<!DOCTYPE html><html lang="zh-CN"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1,maximum-scale=2,viewport-fit=cover"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="renderer" content="webkit"><meta name="author" content="SimonAKing"><link rel="icon" type="image/x-icon" href="https://cdn.jsdelivr.net/gh/SimonAKing/images/blog/favicon.ico"><link rel="shortcut icon" type="image/x-icon" href="https://cdn.jsdelivr.net/gh/SimonAKing/images/blog/favicon.ico"><link rel="apple-touch-icon" href="https://cdn.jsdelivr.net/gh/SimonAKing/images/PWA/apple-touch-icon.png"><title>聊聊大模型推理加速：从数据到系统的技术概要</title><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/SimonAKing/font/comic.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-roboto@1.1.13/index.min.css"><link id="style" rel="stylesheet" href="https://cdn.jsdelivr.net/gh/SimonAKing/blog@7d04115230169/css/style-eaf373873c.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/SimonAKing/blog@7d04115230169/css/post-04c1b3d7ed.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/SimonAKing/blog@7d04115230169/css/module/relatedPosts-1fcaaf0adb.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/SimonAKing/blog@7d04115230169/css/module/comment-2c4172461a.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/firacode@5.2.0/distr/fira_code.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/SimonAKing/blog@7d04115230169/css/highlight-d6d2f7ba38.css"><meta name="theme-color"><meta name="apple-mobile-web-app-status-bar-style"><meta name="msapplication-navbutton-color"><script>
	window.selectedColor = ''
	window.themeColor = localStorage.getItem("theme-color") || "#5275b0"
	window.changeColor = function(themeColor){
		document.documentElement.style.setProperty('--theme-color', themeColor);
		document.querySelector("meta[name=theme-color]").setAttribute("content", themeColor);
		document.querySelector("meta[name=apple-mobile-web-app-status-bar-style]").setAttribute("content", themeColor);
		document.querySelector("meta[name=msapplication-navbutton-color]").setAttribute("content", themeColor);
		localStorage.setItem('theme-color', themeColor)
	}
	changeColor(themeColor)

	window.isPhone = /Android|iOS|iPhone|iPad|iPod|Windows Phone|KFAPWI/i.test(navigator.userAgent) || (window.innerWidth < 1300)
	window.isHome = false
	window.isPost = true
	if(window.isPost){
		window.isReward = true
		window.isWeibo = false
	}
</script><link rel="manifest" href="https://simonaking.com/manifest.json"><meta name="mobile-web-app-capable" content="yes"><meta name="mobile-web-app-title" content="SimonAKing"><meta name="msapplication-starturl" content="https://simonaking.com"><meta name="application-name" content="SimonAKing"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="SimonAKing"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="prefetch" href="https://cdn.jsdelivr.net/"><link rel="dns-prefetch" href="https://api.github.com/"><script async src="https://www.googletagmanager.com/gtag/js?id=UA-109696496-3"></script><script>
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'UA-109696496-3');
</script><link rel="canonical" href="https://simonaking.com/blog/llm-inference-optimization/"><link rel="alternate" type="application/atom+xml" title="SimonAKing" href="/blog/atom.xml"><meta property="og:title" content="聊聊大模型推理加速：从数据到系统的技术概要 | SimonAKing"><meta property="og:site_name" content="SimonAKing"><meta property="og:type" content="article"><meta property="og:url" content="https://simonaking.com/blog/llm-inference-optimization/"><meta property="og:locale" content="zh-CN"><meta name="description" content="随着 LLMs 在各行各业的广泛应用，如何在保证模型性能的前提下实现高效推理已成为重点挑战之一。为了应对这些挑战，学术界和工业界提出了多种优化方案。本文将介绍了若干 LLMs 的推理加速相关的关键技术。如有错误或不准确之处，欢迎指正。 - SimonAKing - SimonAKing"><meta name="keywords" content="LLM,大模型,推理加速,LLM,推理加速,SimonAKing,Blog,博客"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization/111.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization/222.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization/333.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization/4.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization/5.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization/6.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization/7.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization/8.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization/9.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization/10.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization/11.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization/12.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization/13.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization/14.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization/15.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization/16.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization/17.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization/18.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization/19.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization/20.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization/21.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization/22.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization/23.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization/24.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization/25.webp"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization/26.png"><meta property="article:published_time" content="2025-03-27T08:09:40.000Z"><meta property="article:modified_time" content="2025-05-02T08:30:49.023Z"><meta property="og:updated_time" content="2025-05-02T08:30:49.023Z"><meta property="article:author" content="SimonAKing"><meta property="article:tag" content="LLM,大模型,推理加速,LLM,推理加速,SimonAKing,Blog,博客"><meta name="twitter:card" content="summary"><meta name="twitter:site" content="https:&#x2F;&#x2F;simonaking.com&#x2F;blog&#x2F;llm-inference-optimization&#x2F;"><meta name="twitter:creator" content="SimonAKing"><script type="application/ld+json">
{
    "@context": "http://schema.org",
    "url": "https://simonaking.com/blog/llm-inference-optimization/",
    "@type": "BlogPosting",
    "logo": "https://simonaking.com/images/PWA/192.png",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https://simonaking.com/blog/llm-inference-optimization/"
    },
    "headline": "聊聊大模型推理加速：从数据到系统的技术概要 | SimonAKing",
    "image": {
        "@type": "ImageObject",
        "url": "https://simonaking.com/images/PWA/192.png"
    },
    "datePublished": "2025-03-27T08:09:40.000Z",
    "dateModified": "2025-05-02T08:30:49.023Z",
    "author": {
        "@type": "Person",
        "name": "SimonAKing",
        "image": {
            "@type": "ImageObject",
            "url": "https://cdn.jsdelivr.net/gh/SimonAKing/images/blog/avatar.jpg"
        },
        "description": "STDIN | Think &gt;&gt; /dev/Mind"
    },
    "publisher": {
        "@type": "Organization",
        "name": "SimonAKing",
        "logo": {
            "@type": "ImageObject",
            "url": "https://simonaking.com/images/PWA/192.png"
        }
    },
    "keywords": "LLM,大模型,推理加速,LLM,推理加速,SimonAKing,Blog,博客",
    "description": "随着 LLMs 在各行各业的广泛应用，如何在保证模型性能的前提下实现高效推理已成为重点挑战之一。为了应对这些挑战，学术界和工业界提出了多种优化方案。本文将介绍了若干 LLMs 的推理加速相关的关键技术。如有错误或不准确之处，欢迎指正。 - SimonAKing - SimonAKing"
}
</script><!--[if lt IE 9]><style> .alert { padding: 15px; margin-bottom: 20px; border: 1px solid transparent; border-radius: 4px } .alert-danger { background-color: #f2dede; border-color: #ebccd1; color: #a94442; border-bottom: 1px solid #ebccd1 } .alert-link { color: #843534; font-weight: bold } .topframe { margin: 0; padding-left: 15px; padding-right: 15px; text-align: center; border-radius: 0; position: fixed; left: 0; right: 0; top: 0; z-index: 1000 } </style><div class="alert alert-danger topframe"> 你的浏览器实在<strong>太太太太太太旧了</strong>，放学别走，升级完浏览器再说！ <a target="_blank" class="alert-link" href="//browsehappy.com">立即升级</a></div><script src="https://cdn.bootcss.com/html5shiv/r29/html5.min.js"></script><script src="https://cdn.bootcss.com/respond.js/1.4.2/respond.min.js"></script><![endif]--></head><body itemscope itemtype="http://schema.org/WebPage"><aside id="menu" class="hide"><div class="inner flex-row-vertical"><div class="brand-wrap" itemprop="author" itemscope itemtype="http://schema.org/Person"><div class="brand fade"> <a class="avatar waves-effect waves-circle waves-light"><img src="https://cdn.jsdelivr.net/gh/SimonAKing/images/blog/avatar.jpg" title="avatar" alt="avatar" itemprop="image"></a><div class="introduce"><h5 class="nickname " id="name">SimonAKing</h5><div class="links-of-author fade"><span class="links-of-author-item" id="link-wechat"><a href="javascript:void(0)" rel="noopener noreferrer" title="Wechat"><i class="icon icon-lg icon-wechat"></i></a></span><span class="links-of-author-item"><a href="mailto:hi@simonaking.com" rel="noopener noreferrer" target="_blank" title="Email"><i class="icon icon-lg icon-email"></i></a></span><span class="links-of-author-item"><a href="https://x.com/simon_aking" rel="external nofollow noopener noreferrer" target="_blank" title="X"><i class="icon icon-lg icon-XTwitter"></i></a></span><span class="links-of-author-item"><a href="https://github.com/SimonAKing" rel="external nofollow noopener noreferrer" target="_blank" title="Github"><i class="icon icon-lg icon-github"></i></a></span></div><div class="statistics"><ul><li><a class="total-link" href="/blog/weibo/"><div class="count" id="weibo-count">∞</div><div class="type">微博</div></a></li><li><a class="total-link" href="/blog/archives/"><div class="count">32</div><div class="type">文章</div></a></li><li><a class="total-link" href="/gallery/"><div class="count" id="photo-count">∞</div><div class="type">相册</div></a></li></ul></div></div></div></div><div class="scroll-wrap flex-col"><ul class="nav fade"><li class="items waves-block waves-effect"><a href="/blog/"><i class="icon icon-lg icon-xiazai45"></i> <span style="font-size:1em">主 页</span><i class="icon icon-lg icon-chevronleft custom-caret-left"></i></a></li><li class="items waves-block waves-effect"><a href="/blog/archives/"><i class="icon icon-lg icon-guidangxiangmu"></i> <span style="font-size:1em">归 档</span><i class="icon icon-lg icon-chevronleft custom-caret-left"></i></a></li><li class="items waves-block waves-effect"><a href="/blog/tags/"><i class="icon icon-lg icon-biaoqian"></i> <span style="font-size:1em">标 签</span><i class="icon icon-lg icon-chevronleft custom-caret-left"></i></a></li><li class="items waves-block waves-effect"><a href="/blog/weibo/"><i class="icon icon-lg icon-biaoqing"></i> <span style="font-size:1em">微 博</span><i class="icon icon-lg icon-chevronleft custom-caret-left"></i></a></li><li class="items waves-block waves-effect"><a href="//simonaking.com/gallery/"><i class="icon icon-lg icon-xiangce"></i> <span style="font-size:1em">相 册</span><i class="icon icon-lg icon-chevronleft custom-caret-left"></i></a></li><li class="items waves-block waves-effect"><a href="//simonaking.com/about/"><i class="icon icon-lg icon-zhifeiji"></i> <span style="font-size:1em">关 于</span><i class="icon icon-lg icon-chevronleft custom-caret-left"></i></a></li><li class="items waves-block waves-effect"><a href="//simonaking.com/projects/"><i class="icon icon-lg icon-projects"></i> <span style="font-size:1em">项 目</span><i class="icon icon-lg icon-chevronleft custom-caret-left"></i></a></li><li class="items waves-block waves-effect"><a href="//thinking.simonaking.com/" target="_blank" rel="noopener"><i class="icon icon-lg icon--Idea"></i> <span style="font-size:1em">思 考</span><i class="icon icon-lg icon-chevronleft custom-caret-left"></i></a></li><li class="items waves-block waves-effect"><a href="//simonaking.com"><i class="icon icon-lg icon-icon--"></i> <span style="font-size:1em">导航</span><i class="icon icon-lg icon-chevronleft custom-caret-left"></i></a></li><div class="sliding-bar"></div></ul><div class="nav-tool"><a class="nav-tool-item exchange" data-title="简繁互换" href="javascript:translatePage();"><i class="fade icon icon-lg icon-zhuanhuan custom-exchange"></i></a><a class="nav-tool-item picker" data-title="调色板" id="color-picker-icon"><i class="fade icon icon-lg icon-color-palette-outlin custom-picker"></i></a><a class="nav-tool-item light" data-title="关灯" href="javascript:switchNightMode();"><i class="fade icon icon-lg icon-lightbulbo custom-lightbulb"></i></a></div></div></div></aside><main id="main" class="in_post"><header class="top-header in_post" id="header" itemscope itemtype="http://schema.org/WPHeader"><div class="flex-row"><a href="javascript:;" class="header-icon waves-effect waves-circle waves-light on" id="menu-toggle"><i class="icon icon-lg icon-naviconround"></i></a><div class="flex-col"></div><div class="search-wrap" id="search-wrap"><a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="back"><i class="icon icon-lg icon-chevronleft"></i></a> <input type="text" id="key" aria-label="search" class="search-input" autocomplete="off" placeholder="输入感兴趣的关键字"><a href="javascript:;" title="搜索" class="header-icon waves-effect waves-circle waves-light" id="search"><i class="icon icon-lg icon-search"></i></a></div></div><div class="header-title ellipsis">聊聊大模型推理加速：从数据到系统的技术概要</div></header><header class="content-header post-header"><canvas class="flickering-grid-canvas"></canvas></header><div class="container body-wrap"><aside class="post-widget"><nav class="post-toc-wrap fade" id="post-toc"><ol class="post-toc"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#前言"><span class="post-toc-number">1.</span> <span class="post-toc-text">前言</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#正文"><span class="post-toc-number">2.</span> <span class="post-toc-text">正文</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#推理阶段概述"><span class="post-toc-number">2.1.</span> <span class="post-toc-text">推理阶段概述</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#技术总览"><span class="post-toc-number">2.2.</span> <span class="post-toc-text">技术总览</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#数据层面"><span class="post-toc-number">2.2.1.</span> <span class="post-toc-text">数据层面</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#输入压缩"><span class="post-toc-number">2.2.1.1.</span> <span class="post-toc-text">输入压缩</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#输出规划"><span class="post-toc-number">2.2.1.2.</span> <span class="post-toc-text">输出规划</span></a></li></ol></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#模型层面"><span class="post-toc-number">2.2.2.</span> <span class="post-toc-text">模型层面</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#模型结构优化"><span class="post-toc-number">2.2.2.1.</span> <span class="post-toc-text">模型结构优化</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#模型压缩"><span class="post-toc-number">2.2.2.2.</span> <span class="post-toc-text">模型压缩</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#解码优化"><span class="post-toc-number">2.2.2.3.</span> <span class="post-toc-text">解码优化</span></a></li></ol></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#系统层面"><span class="post-toc-number">2.2.3.</span> <span class="post-toc-text">系统层面</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#KV-Cache"><span class="post-toc-number">2.2.3.1.</span> <span class="post-toc-text">KV Cache</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#PagedAttention"><span class="post-toc-number">2.2.3.2.</span> <span class="post-toc-text">PagedAttention</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#Perfix-Cache"><span class="post-toc-number">2.2.3.3.</span> <span class="post-toc-text">Perfix Cache</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#其他"><span class="post-toc-number">2.2.3.4.</span> <span class="post-toc-text">其他</span></a></li></ol></li></ol></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#结束语"><span class="post-toc-number">3.</span> <span class="post-toc-text">结束语</span></a></li></ol></nav></aside><article id="post-聊聊大模型推理加速-从数据到系统的技术概要" class="post-article article-type-post fade" itemscope itemtype="http://schema.org/Article"><div class="post-card"><h1 class="post-card-title" itemprop="name headline">&nbsp;</h1><script>
			window.addEventListener('DOMContentLoaded', function (){ var postTitle = document.querySelector('.post-card-title'); var typingbefore = '聊聊大模型推理加速：从数据到系统的技术概要'; var _i = 0; function typetitle() { if (_i <= typingbefore.length) { postTitle.innerHTML = `${typingbefore.slice(0, _i++)}|`; setTimeout(typetitle, 120); } else { postTitle.innerHTML = typingbefore; } } typetitle(); })
		</script><div class="post-meta"><div class="post-category-phone"><ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/blog/categories/LLM/">LLM</a></li></ul></div> <time itemprop="dateCreated datePublished" class="post-time" title="2025-03-27 16:09:40" datetime="2025-03-27T08:09:40.000Z">2025-03-27</time></div><link itemprop="mainEntityOfPage" href="https://simonaking.com/blog/llm-inference-optimization/" style="display:none"><div class="post-content" id="post-content" itemprop="articleBody"><p>随着 LLMs 在各行各业的应用，如何在保证模型性能的前提下实现高效推理已成为重点挑战之一。</p><a id="more"></a><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>主流的 LLMs 在算法架构上基于 transformer 的 decoder-only 架构，核心任务是 next token prediction，在推理过程中，模型需要依次生成每个 token，这种自回归生成的特点会增加推理延迟。并且从参数量上模型可分为不同规模，即使是小模型，其对计算资源和内存资源也有较高要求。</p><p>为了应对这些挑战，学术界和工业界提出了多种优化方案。本文将介绍了若干 LLMs 的推理加速相关的关键技术。如有错误或不准确之处，欢迎指正。</p><h2 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h2><h3 id="推理阶段概述"><a href="#推理阶段概述" class="headerlink" title="推理阶段概述"></a>推理阶段概述</h3><p>推理阶段是模型接收输入并产生输出的过程。与训练阶段不同，推理阶段不需要计算梯度和更新参数，只需要完成前向传播计算。</p><p>在推理过程中，由于自回归生成的特点，模型需要逐个生成 token。这导致处理的序列长度会不断增加。例如：</p><ul><li>输入长度为 1000 tokens，需要生成 100 个新 tokens，那么实际计算的序列长度为： 1000 + 1001 + … + 1100 = 106050 个 tokens</li><li><strong>每个新 token 都需要与之前所有 token 计算注意力</strong></li></ul><p>在典型的 decoder-only 架构中，有多层 decoder block，每个 token 在推理时都要依次通过这些 block。每个 block 包含 3 个主要计算模块：<strong>Self-Attention</strong>、<strong>FFN</strong>、Layer Normalization（保证数值稳定性的基础，下文忽视）。</p><figure class="image-bubble"><div class="img-lightbox"><div class="overlay"></div> <img src="https://cdn.jsdelivr.net/gh/SimonAKing/images/loading/4-min.gif" alt="image" title data-original="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization/111.png"></div><div class="image-caption">image</div></figure><p>其模块的执行阶段可分为 prefill 阶段与 decode 阶段：</p><table><thead><tr><th>阶段</th><th>特点</th><th>模块</th></tr></thead><tbody><tr><td>Prefill</td><td>• 计算密集场景：计算复杂度为 O（n2），n*n 的矩阵运算，但可利用硬件的并行处理能力加快<br>• 显存密集场景：占用显存随序列长度增长</td><td>• Self-Attention: 需要计算所有输入 token 之间的注意力分数，生成并存储所有 token 的 K、V 到 cache 中<br>• FFN: 对所有输入 token 执行 FFN 计算</td></tr><tr><td>Decode</td><td>• 计算稳定：向量计算，每个 token 的计算量为 O（n），1*n 的向量计算，延迟主要来自顺序依赖，难以并行化<br>• 显存增长稳定</td><td>• Self-Attention: 只需为新 token 生成 Q，复用已缓存的 K、V，计算新 token 的注意力分数<br>• FFN: 只对新生成的 token 执行 FFN 计算</td></tr></tbody></table><blockquote><p>Demonstration of the prefilling stage (a) and decoding stage (b)</p></blockquote><figure class="image-bubble"><div class="img-lightbox"><div class="overlay"></div> <img src="https://cdn.jsdelivr.net/gh/SimonAKing/images/loading/2-min.gif" alt="image" title data-original="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization/222.png"></div><div class="image-caption">image</div></figure><blockquote><p>Illustration of the memory variation through time (latency) during</p></blockquote><figure class="image-bubble"><div class="img-lightbox"><div class="overlay"></div> <img src="https://cdn.jsdelivr.net/gh/SimonAKing/images/loading/2-min.gif" alt="image" title data-original="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization/333.png"></div><div class="image-caption">image</div></figure><p>推理过程中性能指标包括：</p><ul><li>TTFT（Time-To-First-Token）：首 token 的生成时间，主要衡量 Prefill 阶段性能。</li><li>TPOT（Time-Per-Output-Token）：生成每个 token 的时间，主要衡量 Decode 阶段性能。</li></ul><p>通过一个实际的例子来理解推理过程，query：”最近发生了哪些与 Deepseek R1 相关的实践”</p><table><thead><tr><th>阶段</th><th>模块</th><th>处理过程</th><th>结果</th></tr></thead><tbody><tr><td>Prefill</td><td>Self-Attention</td><td>“发生”、”实践” 会与 “Deepseek R1” 产生比较高的关注度，产生 n * n 的注意力矩阵</td><td>输出第一个概率最高的 token</td></tr><tr><td></td><td>FFN</td><td>会在大量已学习知识中，查找到关于 Deepseek 的 pretrain 知识</td><td></td></tr><tr><td>Decode</td><td>Self-Attention</td><td>用生成的 新 token q 去理解输入的 kv ，发现问的是实践，理解需要实践的相关知识</td><td>输出后续概率最高的 token</td></tr><tr><td></td><td>FFN</td><td>会在大量已学习知识中，找到实践的相关知识</td></tr></tbody></table><p>综上，推理阶段的关键开销：</p><ol><li>计算成本：<ol><li>Prefill 要处理所有输入，输入与计算量成平方关系；</li><li>模型参数量越来越大；</li><li>Decode gpu 计算量低</li></ol></li><li>存储成本：<ol><li>KV Cache 与输入序列的长度成正比，需要在显存中保存所有历史 token 的 K/V</li><li>FNN 存储开销大，参数量预计占比 70% 以上</li></ol></li><li>IO 成本：decode 每次只生成一个 token，但要反复调用模型</li></ol><h3 id="技术总览"><a href="#技术总览" class="headerlink" title="技术总览"></a>技术总览</h3><p>针对这些挑战，业界从不同阶段提出了优化方案：</p><figure class="image-bubble"><div class="img-lightbox"><div class="overlay"></div> <img src="https://cdn.jsdelivr.net/gh/SimonAKing/images/loading/4-min.gif" alt="image" title data-original="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization/4.png"></div><div class="image-caption">image</div></figure><h4 id="数据层面"><a href="#数据层面" class="headerlink" title="数据层面"></a>数据层面</h4><h5 id="输入压缩"><a href="#输入压缩" class="headerlink" title="输入压缩"></a>输入压缩</h5><p>在尽可能降低性能影响的前提下，<strong>压缩输入的文本，裁剪低质信息</strong>，从而提升性能，细分工作：</p><ol><li><a href="https://llmlingua.com/" target="_blank" rel="external nofollow noopener noreferrer">LLMLingua Series</a>：训练小模型对原始 prompt 进行裁剪压缩<div class="image-row"><figure class="image-bubble row-image"><div class="img-lightbox"><div class="overlay"></div> <img src="https://cdn.jsdelivr.net/gh/SimonAKing/images/loading/2-min.gif" alt="image" title data-original="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization/5.png"></div><div class="image-caption">image</div></figure><figure class="image-bubble row-image"><div class="img-lightbox"><div class="overlay"></div> <img src="https://cdn.jsdelivr.net/gh/SimonAKing/images/loading/4-min.gif" alt="image" title data-original="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization/6.png"></div><div class="image-caption">image</div></figure></div> 其思路是将压缩过程转化为一个词元分类问题，即决定每个词元是保留还是舍弃。任务模型是通过大模型蒸馏出数据集后，训练一个小模型来执行。<figure class="image-bubble"><div class="img-lightbox"><div class="overlay"></div> <img src="https://cdn.jsdelivr.net/gh/SimonAKing/images/loading/2-min.gif" alt="image" title data-original="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization/7.png"></div><div class="image-caption">image</div></figure> 另一些在 rag 场景的压缩工作：</li><li><p><a href="https://arxiv.org/abs/2310.04408" target="_blank" rel="external nofollow noopener noreferrer">RECOMP</a>：将检索到的文档压缩成文本摘要来提高语言模型的性能，同时减少计算成本。</p><figure class="image-bubble"><div class="img-lightbox"><div class="overlay"></div> <img src="https://cdn.jsdelivr.net/gh/SimonAKing/images/loading/4-min.gif" alt="image" title data-original="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization/8.png"></div><div class="image-caption">image</div></figure></li><li><p><a href="https://arxiv.org/abs/2405.13792" target="_blank" rel="external nofollow noopener noreferrer">xRAG</a>：通过模态融合的方式将文档的 embedding 直接投影到 LLMs 的表示空间中</p><figure class="image-bubble"><div class="img-lightbox"><div class="overlay"></div> <img src="https://cdn.jsdelivr.net/gh/SimonAKing/images/loading/2-min.gif" alt="image" title data-original="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization/9.png"></div><div class="image-caption">image</div></figure></li><li><p>常规的 RAG、Rerank：适用于信息密集型场景，选择高相关的内容，避免 garbage in garbage out</p><figure class="image-bubble"><div class="img-lightbox"><div class="overlay"></div> <img src="https://cdn.jsdelivr.net/gh/SimonAKing/images/loading/1-min.gif" alt="image" title data-original="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization/10.png"></div><div class="image-caption">image</div></figure><p></p><blockquote><p>不相关段落对于 RAG 效果的影响 <a href="https://arxiv.org/pdf/2410.05983" target="_blank" rel="external nofollow noopener noreferrer">https://arxiv.org/pdf/2410.05983</a></p></blockquote></li></ol><h5 id="输出规划"><a href="#输出规划" class="headerlink" title="输出规划"></a>输出规划</h5><p>在尽可能降低性能影响的前提下，<strong>提升长文本输出的并行度</strong>，从而提升性能，细分工作：</p><ol><li><p><a href="https://arxiv.org/abs/2307.15337" target="_blank" rel="external nofollow noopener noreferrer">SOT</a>：SOT 提出一种意图理解后并行输出的思路，对答案生成大纲，然后针对大纲中的每一个点再并行生成，从而提高了线性解码的性能，但对应不同点之间存在依赖的场景，无法很好的解决。所以基于此，又额外训练了个 router model 去判断是否存在依赖，存在依赖则不进行并行解码。</p><div class="image-row"><figure class="image-bubble row-image"><div class="img-lightbox"><div class="overlay"></div> <img src="https://cdn.jsdelivr.net/gh/SimonAKing/images/loading/3-min.gif" alt="image" title data-original="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization/11.png"></div><div class="image-caption">image</div></figure><figure class="image-bubble row-image"><div class="img-lightbox"><div class="overlay"></div> <img src="https://cdn.jsdelivr.net/gh/SimonAKing/images/loading/1-min.gif" alt="image" title data-original="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization/12.png"></div><div class="image-caption">image</div></figure></div></li><li><p><a href="https://arxiv.org/abs/2402.12280" target="_blank" rel="external nofollow noopener noreferrer">SGD</a>：在 SOT 的基础上更进一步，将依赖关系抽象成 DAG（有依赖的点 存在连线；没有依赖的点独立），所以可以更进一步提升效果。</p><figure class="image-bubble"><div class="img-lightbox"><div class="overlay"></div> <img src="https://cdn.jsdelivr.net/gh/SimonAKing/images/loading/3-min.gif" alt="image" title data-original="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization/13.png"></div><div class="image-caption">image</div></figure></li></ol><h4 id="模型层面"><a href="#模型层面" class="headerlink" title="模型层面"></a>模型层面</h4><h5 id="模型结构优化"><a href="#模型结构优化" class="headerlink" title="模型结构优化"></a>模型结构优化</h5><ol><li><p>Attention 优化：核心是减少 KV Cache 以及核函数的运算，</p><blockquote><p>补一下苏神的解答：为什么降低 KV Cache 的大小如此重要？众所周知，一般情况下 LLM 的推理都是在 GPU 上进行，单张 GPU 的显存是有限的，一部分我们要用来存放模型的参数和前向计算的激活值，这部分依赖于模型的体量，选定模型后它就是个常数；另外一部分我们要用来存放模型的 KV Cache，这部分不仅依赖于模型的体量，还依赖于模型的输入长度，也就是在推理过程中是动态增长的，当 Context 长度足够长时，它的大小就会占主导地位，可能超出一张卡甚至一台机（8 张卡）的总显存量。在 GPU 上部署模型的原则是：能一张卡部署的，就不要跨多张卡；能一台机部署的，就不要跨多台机。这是因为“卡内通信带宽 &gt; 卡间通信带宽 &gt; 机间通信带宽”，由于“木桶效应”，模型部署时跨的设备越多，受设备间通信带宽的的“拖累”就越大，事实上即便是单卡 H100 内 SRAM 与 HBM 的带宽已经达到了 3TB/s，但对于 Short Context 来说这个速度依然还是推理的瓶颈，更不用说更慢的卡间、机间通信了。所以，减少 KV Cache 的根本目的是实现在更少的设备上推理更长的 Context，从而实现更快的推理速度以及更低的推理成本。</p></blockquote><ol><li>MHA（Multi-Head Attention）：最基础的多头注意力机制，每个注意力头都有独立的 Q、K、V 参数矩阵。虽然可以充分捕获不同特征维度的信息，但需要存储所有头的 KV Cache，存储开销较大。</li><li>MQA（Multi-Query Attention）: 在计算注意力时，共享的 K/V 会被广播到每个注意力头，然后每个头仍然使用自己独立的 Q 与共享的 K/V 计算注意力分数。KV Cache 大小减少为原来的 1/h （h 是头数）。但共享 KV 可能会限制模型捕获多样化特征的能力。</li><li>GQA（Grouped-Query Attention）: MQA 和 MHA 的折中方案，将注意力头分成若干组，同一组内的头共享 KV。相比 MQA 能保持更好的模型性能，同时相比 MHA 显著减少了 KV Cache。代表作：<a href="https://arxiv.org/pdf/2401.02954" target="_blank" rel="external nofollow noopener noreferrer">DeepSeek V1</a>、<a href="https://scontent-nrt1-2.xx.fbcdn.net/v/t39.2365-6/453304228_1160109801904614_7143520450792086005_n.pdf?_nc_cat=108&amp;ccb=1-7&amp;_nc_sid=3c67a6&amp;_nc_ohc=ivumNlLcSY8Q7kNvgEYKQgM&amp;_nc_ht=scontent-nrt1-2.xx&amp;oh=00_AYDWnrIK1XXJ_bo1ueusjYFZjgJALgata0ZSOFgV6qJHHA&amp;oe=66BA8087" target="_blank" rel="external nofollow noopener noreferrer">LLaMa 3.1</a>、<a href="https://arxiv.org/pdf/2407.10671" target="_blank" rel="external nofollow noopener noreferrer">Qwen2</a>。</li><li>MLA（Multi-head Latent Attention）: 是 <a href="https://arxiv.org/pdf/2405.04434" target="_blank" rel="external nofollow noopener noreferrer">DeepSeek V2</a> 通过引入隐变量的一种新型的注意力机制。它通过压缩键 （Key） 和值 （Value） 到一个低维隐式空间，显著减少了 KV 缓存的大小。与标准注意力机制相比，据称可减少约 93.3% 的 KV 缓存。<div class="image-row"><figure class="image-bubble row-image"><div class="img-lightbox"><div class="overlay"></div> <img src="https://cdn.jsdelivr.net/gh/SimonAKing/images/loading/1-min.gif" alt="image" title data-original="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization/14.png"></div><div class="image-caption">image</div></figure><figure class="image-bubble row-image"><div class="img-lightbox"><div class="overlay"></div> <img src="https://cdn.jsdelivr.net/gh/SimonAKing/images/loading/2-min.gif" alt="image" title data-original="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization/15.png"></div><div class="image-caption">image</div></figure></div></li></ol></li><li><p>MOE（Mixture-of-Expert）：高效的 FFN 设计，将 FFN 拆解成一个 gate network 与 多个 expert，每个 token 只分配给若干个 experts 进行计算，可以显著减少缓存、IO 开销，该思路起源于 1991 年的论文 <a href="https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf" target="_blank" rel="external nofollow noopener noreferrer">Adaptive Mixture of Local Experts</a>。 从 Mistral 的 Mixtral 8x7B 开始，在 24 年 moe 已成为各大模型的主流趋势，以 deepseek 为代表，三个版本的 paper 都是沿着更专业、更极致 的 moe 架构演进。其中的难点有：token 分配的问题、专家学识不精等。</p><div class="image-row"><figure class="image-bubble row-image"><div class="img-lightbox"><div class="overlay"></div> <img src="https://cdn.jsdelivr.net/gh/SimonAKing/images/loading/3-min.gif" alt="image" title data-original="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization/16.png"></div><div class="image-caption">image</div></figure><figure class="image-bubble row-image"><div class="img-lightbox"><div class="overlay"></div> <img src="https://cdn.jsdelivr.net/gh/SimonAKing/images/loading/1-min.gif" alt="image" title data-original="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization/17.png"></div><div class="image-caption">image</div></figure></div></li></ol><h5 id="模型压缩"><a href="#模型压缩" class="headerlink" title="模型压缩"></a>模型压缩</h5><p>模型压缩旨在通过降低模型复杂度（包括参数量、计算量和内存占用）来减少资源开销，在尽可能保持模型性能的前提下提升推理效率。其中一些代表性的措施有：</p><figure class="image-bubble"><div class="img-lightbox"><div class="overlay"></div> <img src="https://cdn.jsdelivr.net/gh/SimonAKing/images/loading/1-min.gif" alt="image" title data-original="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization/18.png"></div><div class="image-caption">image</div></figure><ul><li><p>量化：量化是降低模型权重和激活的精度，大多数模型都以 32 或 16 位精度进行训练，其中每个参数和激活元素占用 32 或 16 位内存（单精度浮点），显然，可以尝试使用更少的位来表示权重。 常见的方法有训练后量化，模型训练完直接转换数值精度，操作简单但精度可能下降；还有量化感知训练，训练时模拟量化效果，让模型提前适应低精度，精度损失更小。收益除了大幅减少模型体积外，也能提高计算速度。在 prefill 阶段 重点量化激活值量化；在 decode 阶段 重点量化权重。</p><figure class="image-bubble"><div class="img-lightbox"><div class="overlay"></div> <img src="https://cdn.jsdelivr.net/gh/SimonAKing/images/loading/2-min.gif" alt="image" title data-original="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization/19.png"></div><div class="image-caption">image</div></figure></li><li><p><a href="https://arxiv.org/pdf/1503.02531" target="_blank" rel="external nofollow noopener noreferrer">知识蒸馏</a>：知识蒸馏是将大模型（教师模型）的知识转移到小模型（学生模型）中的压缩方法。常见方法有训练后蒸馏，直接用教师模型生成数据训练学生模型，可以在保持大部分性能的同时将模型压缩至原来的 40-60%。</p><figure class="image-bubble"><div class="img-lightbox"><div class="overlay"></div> <img src="https://cdn.jsdelivr.net/gh/SimonAKing/images/loading/1-min.gif" alt="image" title data-original="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization/20.png"></div><div class="image-caption">image</div></figure></li><li><p>减枝：通过移除模型中不重要的连接或结构来压缩模型。常见方法有结构化剪枝（移除整层或整个头）和非结构化剪枝（移除单个权重），可以在性能损失较小的情况下减少 30-50% 的参数量。</p></li><li><p>低秩分解：将大的权重矩阵分解成若干个小矩阵的乘积，从而减少参数量和计算量。常见方法有 SVD 分解和 LoRA 等。其中 LoRA 通过在原始权重旁增加低秩矩阵来实现参数高效微调，已成为主流的模型压缩方法之一。</p></li></ul><h5 id="解码优化"><a href="#解码优化" class="headerlink" title="解码优化"></a>解码优化</h5><p>推理阶段最耗时的部分是自回归生成过程，解码优化旨在通过并行化或预测等方式来加速 token 生成。其中一些代表性的措施有：</p><ol><li><p><a href="https://arxiv.org/abs/2302.01318" target="_blank" rel="external nofollow noopener noreferrer">推测解码 （Speculative Decoding）</a>：一种用于自回归大模型的创新解码技术，旨在提高解码效率，同时不影响输出的质量。这种方法的核心思想包括使用一个较小的模型（Draft Model）来有效地预测几个后续 token，然后使用大模型并行验证这些预测。该方法旨在使大模型能够在单个推理通常所需的 a 时间范围内生成多个 token。</p><figure class="image-bubble"><div class="img-lightbox"><div class="overlay"></div> <img src="https://cdn.jsdelivr.net/gh/SimonAKing/images/loading/4-min.gif" alt="image" title data-original="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization/21.png"></div><div class="image-caption">image</div></figure></li><li><p>跳跃式解码：代表工作 SGLang，核心思想是通过分析文本结构的确定性特征，在生成过程中跳过可预测的部分。它不需要逐个 token 地生成，而是能够在确定性很强的位置直接跳转到下一个需要推理的位置，从而大幅提升生成效率。</p><figure class="image-bubble"><div class="img-lightbox"><div class="overlay"></div> <img src="https://cdn.jsdelivr.net/gh/SimonAKing/images/loading/3-min.gif" alt="image" title data-original="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization/22.png"></div><div class="image-caption">image</div></figure></li><li><p>约束性解码：代表工作 Outlines，Outlines 通过将 JSON Schema 转换成正则表达式，然后基于该正则表达式构建有限状态机（FSM， Finite State Machine），然后在生成过程中实时过滤 token，确保输出始终符合预定义的格式要求。这种方式特别适合需要严格控制输出格式的场景。</p><figure class="image-bubble"><div class="img-lightbox"><div class="overlay"></div> <img src="https://cdn.jsdelivr.net/gh/SimonAKing/images/loading/3-min.gif" alt="image" title data-original="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization/23.png"></div><div class="image-caption">image</div></figure></li><li><p>结构化解码：代表工作 Guidance，创新点在于将生成任务分为固定结构和动态内容两类，并采用不同的处理策略。通过识别文本中的模板部分，可以显著减少需要实际生成的内容量，从而提高整体效率。</p><figure class="image-bubble"><div class="img-lightbox"><div class="overlay"></div> <img src="https://cdn.jsdelivr.net/gh/SimonAKing/images/loading/2-min.gif" alt="image" title data-original="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization/24.png"></div><div class="image-caption">image</div></figure></li></ol><h4 id="系统层面"><a href="#系统层面" class="headerlink" title="系统层面"></a>系统层面</h4><h5 id="KV-Cache"><a href="#KV-Cache" class="headerlink" title="KV Cache"></a>KV Cache</h5><p>KV Cache 在 LLM 推理过程中占用了大量访存带宽，这使得 KV Cache 的压缩优化成为 LLM 推理领域的研究热点。目前主要有两种压缩技术路线：</p><ol><li>稀疏化压缩通过减少保留的关键 token 数量来实现压缩。例子：<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">原始输入：</span><br><span class="line">[T1, T2, T3, ..., T128K]  // 128K tokens 的 prompt</span><br><span class="line">稀疏化后：</span><br><span class="line">[T1, T512, T1024, ..., T128K]  // 仅保留 1024 个关键 tokens</span><br></pre></td></tr></tbody></table></figure></li><li>量化压缩在保持 token 数量不变的情况下，降低数据精度。例子： 这两种方法各有特点，但都致力于在保证模型性能的前提下，降低内存占用并提升推理速度。稀疏化适合处理超长文本输入，而量化则是一种普适性的优化方案。 KV Cache 优化的过程：<figure class="image-bubble"><div class="img-lightbox"><div class="overlay"></div> <img src="https://cdn.jsdelivr.net/gh/SimonAKing/images/loading/4-min.gif" alt="image" title data-original="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization/25.webp"></div><div class="image-caption">image</div></figure></li></ol><h5 id="PagedAttention"><a href="#PagedAttention" class="headerlink" title="PagedAttention"></a>PagedAttention</h5><p>传统推理框架采用静态显存分配策略：按照 batch_size × max_seq_len 预分配固定大小的显存块，这导致显存利用率低下。</p><figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">// 传统静态分配</span><br><span class="line">预分配显存大小 = 4(batch_size) × 2048(max_seq_len) × 2(bytes/token)</span><br><span class="line">实际使用情况：</span><br><span class="line">- 请求 1: 使用 256 tokens</span><br><span class="line">- 请求 2: 使用 512 tokens</span><br><span class="line">- 请求 3: 使用 128 tokens</span><br><span class="line">结果：大量显存空间被浪费</span><br></pre></td></tr></tbody></table></figure> vllm 借鉴操作系统的虚拟内存分页思想，提出了 PagedAttention 技术，实现 KV Cache 显存的动态分配。<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">// PagedAttention 动态分配</span><br><span class="line">显存页大小 = 16KB</span><br><span class="line">请求 1: 动态分配 2 页 (32KB)</span><br><span class="line">请求 2: 动态分配 4 页 (64KB)</span><br><span class="line">请求 3: 动态分配 1 页 (16KB)</span><br><span class="line">优势：按需分配，提高显存利用率</span><br></pre></td></tr></tbody></table></figure> 这种动态管理机制让显存资源得到更高效的利用，特别适合处理长度变化较大的多请求场景。<p></p><figure class="image-bubble"><div class="img-lightbox"><div class="overlay"></div> <img src="https://cdn.jsdelivr.net/gh/SimonAKing/images/loading/1-min.gif" alt="image" title data-original="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization/26.png"></div><div class="image-caption">image</div></figure><h5 id="Perfix-Cache"><a href="#Perfix-Cache" class="headerlink" title="Perfix Cache"></a>Perfix Cache</h5><p>传统 PagedAttention 的局限：KV Cache 只能在单个请求内复用，无法跨请求共享，这在多轮对话场景下效率不高。 例子：</p><figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">// 传统方式</span><br><span class="line">对话轮次 1：</span><br><span class="line">  Prompt: "你好，请介绍下北京"</span><br><span class="line">  → 计算完整的 KV Cache</span><br><span class="line"></span><br><span class="line">对话轮次 2：</span><br><span class="line">  Prompt: "你好，请介绍下北京的故宫"</span><br><span class="line">  → 重新计算 KV Cache（即使包含大量重复内容）</span><br></pre></td></tr></tbody></table></figure> Prefix Caching 优化：通过缓存系统提示和历史对话的 KV Cache，实现跨请求复用，降低首次 Token 生成时间（TTFT）。 SGLang 提出的 RadixAttention 通过三个关键步骤实现自动 KV Cache 复用：<p></p><ol><li>缓存保留<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">请求完成后：</span><br><span class="line">KV Cache → 保留在 GPU 显存</span><br><span class="line">Token 序列 → 存入 RadixTree</span><br></pre></td></tr></tbody></table></figure></li><li>前缀匹配<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">新请求："你好，请介绍下北京的故宫"</span><br><span class="line">↓</span><br><span class="line">RadixTree 匹配</span><br><span class="line">↓</span><br><span class="line">命中："你好，请介绍下北京"的 KV Cache</span><br><span class="line">↓</span><br><span class="line">仅需计算新增部分："的故宫"</span><br></pre></td></tr></tbody></table></figure></li><li>缓存管理<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">策略：LRU（最近最少使用）</span><br><span class="line">当显存不足时：</span><br><span class="line">- 识别最久未使用的 KV Cache</span><br><span class="line">- 优先释放这部分显存</span><br></pre></td></tr></tbody></table></figure> 这种优化特别适合多轮对话等场景，可以显著提升模型响应速度和资源利用效率。</li></ol><h5 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h5><p>系统层面还有诸多优化手段，如张量并行、流水线并行、分布式推理等。由于这些优化方案涉及较为复杂的系统架构设计，在此不做展开讨论。感兴趣的读者可以参考相关技术文档深入了解。</p><hr><h2 id="结束语"><a href="#结束语" class="headerlink" title="结束语"></a>结束语</h2><p>通过对 LLMs 推理加速技术的深入探讨，我们可以看到：</p><ol><li>技术架构与挑战<ul><li>推理过程分为 Prefill 和 Decode 两个主要阶段</li><li>核心挑战：<ul><li>计算成本：输入规模、参数量、自回归特性</li><li>存储成本：KV Cache、FFN 参数存储</li><li>IO 成本：频繁模型调用</li></ul></li></ul></li><li>优化方案（多层次技术栈）<ul><li>数据层面优化：<ul><li>输入压缩：LLMLingua、RECOMP、xRAG 等</li><li>输出规划：SOT、SGD 等并行输出技术</li></ul></li><li>模型层面优化：<ul><li>结构优化：注意力机制（MHA、MQA、GQA、MLA）、MOE 架构</li><li>模型压缩：量化、知识蒸馏、减枝、低秩分解</li><li>解码优化：推测解码、跳跃式解码、约束性解码等</li></ul></li><li>系统层面优化：<ul><li>KV Cache 优化：稀疏化压缩、量化压缩</li><li>PagedAttention：动态显存管理</li><li>Prefix Cache：跨请求 KV 复用 随着 LLMs 在各领域的应用不断扩大，推理加速技术也在持续演进。从早期的简单优化到现在的多层次综合优化方案，技术在不断突破。这些优化技术的发展使得 LLMs 能够更高效地服务于实际应用场景，为 AI 应用的普及奠定了重要基础。未来，随着硬件技术的进步和算法的创新，我们有望看到更多突破性的推理加速方案。</li></ul></li></ul></li></ol><p>参考资料：</p><ol><li><a href="https://arxiv.org/abs/2404.14294" target="_blank" rel="external nofollow noopener noreferrer">A Survey on Efficient Inference for Large Language Models</a> 帮助很大，推荐</li><li><a href="https://arxiv.org/abs/2402.09748" target="_blank" rel="external nofollow noopener noreferrer">Model Compression and Efficient Inference for Large Language Models: A Survey</a></li><li><a href="https://developer.nvidia.com/blog/mastering-llm-techniques-inference-optimization/" target="_blank" rel="external nofollow noopener noreferrer">Mastering LLM Techniques: Inference Optimization</a></li><li><a href="https://zhuanlan.zhihu.com/p/689773196" target="_blank" rel="external nofollow noopener noreferrer">AI 大模型推理过程和优化技术</a></li><li><a href="https://vgel.me/posts/faster-inference/" target="_blank" rel="external nofollow noopener noreferrer">How to make LLMs go fast</a></li><li><a href="https://www.yidoo.xyz/" target="_blank" rel="external nofollow noopener noreferrer">异度部落格</a></li><li><a href="https://mp.weixin.qq.com/s/WFJxnTF9fGIIXPA7GQ5V2w" target="_blank" rel="external nofollow noopener noreferrer">详细谈谈 DeepSeek MoE 相关的技术发展</a></li><li><a href="https://mp.weixin.qq.com/s/yCczYU0po0PvPTa-eh2pfg" target="_blank" rel="external nofollow noopener noreferrer">缓存与效果的极限拉扯：从 MHA、MQA、GQA 到 MLA</a></li><li><a href="https://arxiv.org/pdf/2308.07633" target="_blank" rel="external nofollow noopener noreferrer">A Survey on Model Compression for Large Language Models</a></li></ol><p>欢迎转载本站文章，请注明作者和出处 <a href="http://simonaking.com">SimonAKing</a>。</p></div><div class="recommended_posts"><div class="recommended_wrap"><h2 class="recommended_title">相关文章</h2><ul class="popular-posts"><li class="popular-posts-item"><div class="popular-posts-title"> <a href="/blog/llm-inference-optimization-en/" rel="bookmark">LLM Inference Optimization Overview - From Data to System Architecture</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"> <a href="/blog/rerank/" rel="bookmark">聊聊 Rerank：从 BERT 到大模型的技术旅程</a></div></li></ul></div></div><blockquote class="post-copyright"><div class="content"> <span class="post-time">最后更新：<time datetime="2025-05-02T08:30:49.023Z">2025-05-02 16:30:49</time></span> 原文链接：<a href="/blog/llm-inference-optimization/" target="_blank" rel="external">https://simonaking.com/blog/llm-inference-optimization/</a></div><footer><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/blog/tags/LLM/" rel="tag">LLM</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/blog/tags/%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/" rel="tag">推理加速</a></li></ul><div> <img itemprop="image" lass="copyimg" src="https://cdn.jsdelivr.net/gh/SimonAKing/images/blog/avatar.jpg" alt="SimonAKing"> <a itemprop="author" itemscope itemtype="http://schema.org/Person" href="/blog">SimonAKing</a></div></footer></blockquote></div><section class="comments" id="comments"><div id="disqus_thread"></div></section></article></div></main><div class="mask" id="mask"></div><a href="javascript:;" id="gotop" class="fade-scale waves-effect waves-circle waves-light"><span class="icon icon-lg icon-chevronup"></span></a><div class="search-panel in_post" id="search-panel"><ul class="search-result" id="search-result"></ul></div><template id="search-tpl"><li class="item waves-block waves-effect" onclick="location.href='{path}'" data-path="{path}"><div class="title ellipsis" title="{title}">{title}</div></li></template><div id="color-picker" class="page-modal"><a class="close" href="javascript:;"><i class="icon icon-close2"></i></a><br><h4><i class="icon icon-quote-left"></i> <span>RGB调色实验室 🎨</span><i class="icon icon-quote-right"></i></h4><div id="color-picker-container"></div></div><div id="wechat" class="page-modal wechat-wrap"><a class="close" href="javascript:;"><i class="icon icon-close2"></i></a><br><h4 class="wechat-title"> <span style="font-size: 18px">欢迎添加我的微信，进群交流、学习<br><i class="icon icon-quote-left"></i> simonaking<i class="icon icon-quote-right"></i></span></h4> <img id="wechat_img" src="https://cdn.jsdelivr.net/gh/SimonAKing/images/blog/wx.png" alt="欢迎您的关注！" style="margin-top: 5%"></div><script src="https://simonaking.com/registerSW.js?v=7d04115230169"></script><script src="https://cdn.jsdelivr.net/gh/SimonAKing/js/log.min.js"></script><script src="https://cdn.jsdelivr.net/npm/node-waves@0.7.6/src/js/waves.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/headroom.js@0.12.0/dist/headroom.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@jaames/iro@5.2.3/dist/iro.min.js"></script><script src="https://cdn.jsdelivr.net/gh/SimonAKing/blog@7d04115230169/js/particles-585dd6bbf8.js"></script><script src="https://cdn.jsdelivr.net/gh/SimonAKing/blog@7d04115230169/js/main-c5641580d9.js"></script><script src="https://cdn.jsdelivr.net/gh/SimonAKing/blog@7d04115230169/js/script-66d90ba09e.js"></script><script src="https://cdn.jsdelivr.net/gh/SimonAKing/blog@7d04115230169/js/module/hover-effect-e72a114ce3.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@5.1.0/instantpage.min.js"></script><script src="https://cdn.jsdelivr.net/gh/SimonAKing/blog@7d04115230169/js/module/post-6d3f729af5.js"></script><script defer src="https://cdn.jsdelivr.net/gh/SimonAKing/blog@7d04115230169/js/module/comment-5cad405b01.js"></script><script defer src="https://cdn.jsdelivr.net/gh/SimonAKing/js/translate-ff7607fcb8.js"></script></body></html>