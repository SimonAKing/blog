<!DOCTYPE html><html lang="zh-CN"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1,maximum-scale=2,viewport-fit=cover"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="renderer" content="webkit"><meta name="author" content="SimonAKing"><link rel="icon" type="image/x-icon" href="https://cdn.jsdelivr.net/gh/SimonAKing/images/blog/favicon.ico"><link rel="shortcut icon" type="image/x-icon" href="https://cdn.jsdelivr.net/gh/SimonAKing/images/blog/favicon.ico"><link rel="apple-touch-icon" href="https://cdn.jsdelivr.net/gh/SimonAKing/images/PWA/apple-touch-icon.png"><title>LLM Inference Optimization Overview - From Data to System Architecture</title><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/SimonAKing/font/comic.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-roboto@1.1.13/index.min.css"><link id="style" rel="stylesheet" href="https://cdn.jsdelivr.net/gh/SimonAKing/blog@7d04115230169/css/style-eaf373873c.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/SimonAKing/blog@7d04115230169/css/post-04c1b3d7ed.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/SimonAKing/blog@7d04115230169/css/module/relatedPosts-1fcaaf0adb.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/SimonAKing/blog@7d04115230169/css/module/comment-2c4172461a.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/firacode@5.2.0/distr/fira_code.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/SimonAKing/blog@7d04115230169/css/highlight-d6d2f7ba38.css"><meta name="theme-color"><meta name="apple-mobile-web-app-status-bar-style"><meta name="msapplication-navbutton-color"><script>
	window.selectedColor = ''
	window.themeColor = localStorage.getItem("theme-color") || "#5275b0"
	window.changeColor = function(themeColor){
		document.documentElement.style.setProperty('--theme-color', themeColor);
		document.querySelector("meta[name=theme-color]").setAttribute("content", themeColor);
		document.querySelector("meta[name=apple-mobile-web-app-status-bar-style]").setAttribute("content", themeColor);
		document.querySelector("meta[name=msapplication-navbutton-color]").setAttribute("content", themeColor);
		localStorage.setItem('theme-color', themeColor)
	}
	changeColor(themeColor)

	window.isPhone = /Android|iOS|iPhone|iPad|iPod|Windows Phone|KFAPWI/i.test(navigator.userAgent) || (window.innerWidth < 1300)
	window.isHome = false
	window.isPost = true
	if(window.isPost){
		window.isReward = true
		window.isWeibo = false
	}
</script><link rel="manifest" href="https://simonaking.com/manifest.json"><meta name="mobile-web-app-capable" content="yes"><meta name="mobile-web-app-title" content="SimonAKing"><meta name="msapplication-starturl" content="https://simonaking.com"><meta name="application-name" content="SimonAKing"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="SimonAKing"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="prefetch" href="https://cdn.jsdelivr.net/"><link rel="dns-prefetch" href="https://api.github.com/"><script async src="https://www.googletagmanager.com/gtag/js?id=UA-109696496-3"></script><script>
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'UA-109696496-3');
</script><link rel="canonical" href="https://simonaking.com/blog/llm-inference-optimization-en/"><link rel="alternate" type="application/atom+xml" title="SimonAKing" href="/blog/atom.xml"><meta property="og:title" content="LLM Inference Optimization Overview - From Data to System Architecture | SimonAKing"><meta property="og:site_name" content="SimonAKing"><meta property="og:type" content="article"><meta property="og:url" content="https://simonaking.com/blog/llm-inference-optimization-en/"><meta property="og:locale" content="zh-CN"><meta name="description" content="As LLMs are widely adopted across various industries, achieving efficient inference while maintaining model performance has become a key challenge. To address these challenges, academia and industry h - SimonAKing - SimonAKing"><meta name="keywords" content="LLM,Large Language Models,Inference Optimization,LLM,Inference Optimization,SimonAKing,Blog,博客"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization-en/111.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization-en/222.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization-en/333.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization-en/4.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization-en/5.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization-en/6.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization-en/7.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization-en/8.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization-en/9.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization-en/10.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization-en/11.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization-en/12.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization-en/13.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization-en/14.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization-en/15.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization-en/16.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization-en/17.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization-en/18.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization-en/19.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization-en/20.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization-en/21.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization-en/22.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization-en/23.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization-en/24.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization-en/25.webp"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization-en/26.png"><meta property="article:published_time" content="2025-05-05T08:09:40.000Z"><meta property="article:modified_time" content="2025-05-11T15:32:57.684Z"><meta property="og:updated_time" content="2025-05-11T15:32:57.684Z"><meta property="article:author" content="SimonAKing"><meta property="article:tag" content="LLM,Large Language Models,Inference Optimization,LLM,Inference Optimization,SimonAKing,Blog,博客"><meta name="twitter:card" content="summary"><meta name="twitter:site" content="https:&#x2F;&#x2F;simonaking.com&#x2F;blog&#x2F;llm-inference-optimization-en&#x2F;"><meta name="twitter:creator" content="SimonAKing"><script type="application/ld+json">
{
    "@context": "http://schema.org",
    "url": "https://simonaking.com/blog/llm-inference-optimization-en/",
    "@type": "BlogPosting",
    "logo": "https://simonaking.com/images/PWA/192.png",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https://simonaking.com/blog/llm-inference-optimization-en/"
    },
    "headline": "LLM Inference Optimization Overview - From Data to System Architecture | SimonAKing",
    "image": {
        "@type": "ImageObject",
        "url": "https://simonaking.com/images/PWA/192.png"
    },
    "datePublished": "2025-05-05T08:09:40.000Z",
    "dateModified": "2025-05-11T15:32:57.684Z",
    "author": {
        "@type": "Person",
        "name": "SimonAKing",
        "image": {
            "@type": "ImageObject",
            "url": "https://cdn.jsdelivr.net/gh/SimonAKing/images/blog/avatar.jpg"
        },
        "description": "STDIN | Think &gt;&gt; /dev/Mind"
    },
    "publisher": {
        "@type": "Organization",
        "name": "SimonAKing",
        "logo": {
            "@type": "ImageObject",
            "url": "https://simonaking.com/images/PWA/192.png"
        }
    },
    "keywords": "LLM,Large Language Models,Inference Optimization,LLM,Inference Optimization,SimonAKing,Blog,博客",
    "description": "As LLMs are widely adopted across various industries, achieving efficient inference while maintaining model performance has become a key challenge. To address these challenges, academia and industry h - SimonAKing - SimonAKing"
}
</script><!--[if lt IE 9]><style> .alert { padding: 15px; margin-bottom: 20px; border: 1px solid transparent; border-radius: 4px } .alert-danger { background-color: #f2dede; border-color: #ebccd1; color: #a94442; border-bottom: 1px solid #ebccd1 } .alert-link { color: #843534; font-weight: bold } .topframe { margin: 0; padding-left: 15px; padding-right: 15px; text-align: center; border-radius: 0; position: fixed; left: 0; right: 0; top: 0; z-index: 1000 } </style><div class="alert alert-danger topframe"> 你的浏览器实在<strong>太太太太太太旧了</strong>，放学别走，升级完浏览器再说！ <a target="_blank" class="alert-link" href="//browsehappy.com">立即升级</a></div><script src="https://cdn.bootcss.com/html5shiv/r29/html5.min.js"></script><script src="https://cdn.bootcss.com/respond.js/1.4.2/respond.min.js"></script><![endif]--></head><body itemscope itemtype="http://schema.org/WebPage"><aside id="menu" class="hide"><div class="inner flex-row-vertical"><div class="brand-wrap" itemprop="author" itemscope itemtype="http://schema.org/Person"><div class="brand fade"> <a class="avatar waves-effect waves-circle waves-light"><img src="https://cdn.jsdelivr.net/gh/SimonAKing/images/blog/avatar.jpg" title="avatar" alt="avatar" itemprop="image"></a><div class="introduce"><h5 class="nickname " id="name">SimonAKing</h5><div class="links-of-author fade"><span class="links-of-author-item" id="link-wechat"><a href="javascript:void(0)" rel="noopener noreferrer" title="Wechat"><i class="icon icon-lg icon-wechat"></i></a></span><span class="links-of-author-item"><a href="mailto:hi@simonaking.com" rel="noopener noreferrer" target="_blank" title="Email"><i class="icon icon-lg icon-email"></i></a></span><span class="links-of-author-item"><a href="https://x.com/simon_aking" rel="external nofollow noopener noreferrer" target="_blank" title="X"><i class="icon icon-lg icon-XTwitter"></i></a></span><span class="links-of-author-item"><a href="https://github.com/SimonAKing" rel="external nofollow noopener noreferrer" target="_blank" title="Github"><i class="icon icon-lg icon-github"></i></a></span></div><div class="statistics"><ul><li><a class="total-link" href="/blog/weibo/"><div class="count" id="weibo-count">∞</div><div class="type">微博</div></a></li><li><a class="total-link" href="/blog/archives/"><div class="count">32</div><div class="type">文章</div></a></li><li><a class="total-link" href="/gallery/"><div class="count" id="photo-count">∞</div><div class="type">相册</div></a></li></ul></div></div></div></div><div class="scroll-wrap flex-col"><ul class="nav fade"><li class="items waves-block waves-effect"><a href="/blog/"><i class="icon icon-lg icon-xiazai45"></i> <span style="font-size:1em">主 页</span><i class="icon icon-lg icon-chevronleft custom-caret-left"></i></a></li><li class="items waves-block waves-effect"><a href="/blog/archives/"><i class="icon icon-lg icon-guidangxiangmu"></i> <span style="font-size:1em">归 档</span><i class="icon icon-lg icon-chevronleft custom-caret-left"></i></a></li><li class="items waves-block waves-effect"><a href="/blog/tags/"><i class="icon icon-lg icon-biaoqian"></i> <span style="font-size:1em">标 签</span><i class="icon icon-lg icon-chevronleft custom-caret-left"></i></a></li><li class="items waves-block waves-effect"><a href="/blog/weibo/"><i class="icon icon-lg icon-biaoqing"></i> <span style="font-size:1em">微 博</span><i class="icon icon-lg icon-chevronleft custom-caret-left"></i></a></li><li class="items waves-block waves-effect"><a href="//simonaking.com/gallery/"><i class="icon icon-lg icon-xiangce"></i> <span style="font-size:1em">相 册</span><i class="icon icon-lg icon-chevronleft custom-caret-left"></i></a></li><li class="items waves-block waves-effect"><a href="//simonaking.com/about/"><i class="icon icon-lg icon-zhifeiji"></i> <span style="font-size:1em">关 于</span><i class="icon icon-lg icon-chevronleft custom-caret-left"></i></a></li><li class="items waves-block waves-effect"><a href="//simonaking.com/projects/"><i class="icon icon-lg icon-projects"></i> <span style="font-size:1em">项 目</span><i class="icon icon-lg icon-chevronleft custom-caret-left"></i></a></li><li class="items waves-block waves-effect"><a href="//thinking.simonaking.com/" target="_blank" rel="noopener"><i class="icon icon-lg icon--Idea"></i> <span style="font-size:1em">思 考</span><i class="icon icon-lg icon-chevronleft custom-caret-left"></i></a></li><li class="items waves-block waves-effect"><a href="//simonaking.com"><i class="icon icon-lg icon-icon--"></i> <span style="font-size:1em">导航</span><i class="icon icon-lg icon-chevronleft custom-caret-left"></i></a></li><div class="sliding-bar"></div></ul><div class="nav-tool"><a class="nav-tool-item exchange" data-title="简繁互换" href="javascript:translatePage();"><i class="fade icon icon-lg icon-zhuanhuan custom-exchange"></i></a><a class="nav-tool-item picker" data-title="调色板" id="color-picker-icon"><i class="fade icon icon-lg icon-color-palette-outlin custom-picker"></i></a><a class="nav-tool-item light" data-title="关灯" href="javascript:switchNightMode();"><i class="fade icon icon-lg icon-lightbulbo custom-lightbulb"></i></a></div></div></div></aside><main id="main" class="in_post"><header class="top-header in_post" id="header" itemscope itemtype="http://schema.org/WPHeader"><div class="flex-row"><a href="javascript:;" class="header-icon waves-effect waves-circle waves-light on" id="menu-toggle"><i class="icon icon-lg icon-naviconround"></i></a><div class="flex-col"></div><div class="search-wrap" id="search-wrap"><a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="back"><i class="icon icon-lg icon-chevronleft"></i></a> <input type="text" id="key" aria-label="search" class="search-input" autocomplete="off" placeholder="输入感兴趣的关键字"><a href="javascript:;" title="搜索" class="header-icon waves-effect waves-circle waves-light" id="search"><i class="icon icon-lg icon-search"></i></a></div></div><div class="header-title ellipsis">LLM Inference Optimization Overview - From Data to System Architecture</div></header><header class="content-header post-header"><canvas class="flickering-grid-canvas"></canvas></header><div class="container body-wrap"><aside class="post-widget"><nav class="post-toc-wrap fade" id="post-toc"><ol class="post-toc"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Introduction"><span class="post-toc-number">1.</span> <span class="post-toc-text">Introduction</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Main-Content"><span class="post-toc-number">2.</span> <span class="post-toc-text">Main Content</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Inference-Phase-Overview"><span class="post-toc-number">2.1.</span> <span class="post-toc-text">Inference Phase Overview</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Technology-Overview"><span class="post-toc-number">2.2.</span> <span class="post-toc-text">Technology Overview</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Data-Level"><span class="post-toc-number">2.2.1.</span> <span class="post-toc-text">Data Level</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#Input-Compression"><span class="post-toc-number">2.2.1.1.</span> <span class="post-toc-text">Input Compression</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#Output-Planning"><span class="post-toc-number">2.2.1.2.</span> <span class="post-toc-text">Output Planning</span></a></li></ol></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Model-Level"><span class="post-toc-number">2.2.2.</span> <span class="post-toc-text">Model Level</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#Model-Structure-Optimization"><span class="post-toc-number">2.2.2.1.</span> <span class="post-toc-text">Model Structure Optimization</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#Model-Compression"><span class="post-toc-number">2.2.2.2.</span> <span class="post-toc-text">Model Compression</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#Decoding-Optimization"><span class="post-toc-number">2.2.2.3.</span> <span class="post-toc-text">Decoding Optimization</span></a></li></ol></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#System-Level"><span class="post-toc-number">2.2.3.</span> <span class="post-toc-text">System Level</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#KV-Cache"><span class="post-toc-number">2.2.3.1.</span> <span class="post-toc-text">KV Cache</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#PagedAttention"><span class="post-toc-number">2.2.3.2.</span> <span class="post-toc-text">PagedAttention</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#Prefix-Cache"><span class="post-toc-number">2.2.3.3.</span> <span class="post-toc-text">Prefix Cache</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#Others"><span class="post-toc-number">2.2.3.4.</span> <span class="post-toc-text">Others</span></a></li></ol></li></ol></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Conclusion"><span class="post-toc-number">3.</span> <span class="post-toc-text">Conclusion</span></a></li></ol></nav></aside><article id="post-llm-inference-optimization-overview" class="post-article article-type-post fade" itemscope itemtype="http://schema.org/Article"><div class="post-card"><h1 class="post-card-title" itemprop="name headline">&nbsp;</h1><script>
			window.addEventListener('DOMContentLoaded', function (){ var postTitle = document.querySelector('.post-card-title'); var typingbefore = 'LLM Inference Optimization Overview - From Data to System Architecture'; var _i = 0; function typetitle() { if (_i <= typingbefore.length) { postTitle.innerHTML = `${typingbefore.slice(0, _i++)}|`; setTimeout(typetitle, 120); } else { postTitle.innerHTML = typingbefore; } } typetitle(); })
		</script><div class="post-meta"><div class="post-category-phone"><ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/blog/categories/LLM/">LLM</a></li></ul></div> <time itemprop="dateCreated datePublished" class="post-time" title="2025-05-05 16:09:40" datetime="2025-05-05T08:09:40.000Z">2025-05-05</time></div><link itemprop="mainEntityOfPage" href="https://simonaking.com/blog/llm-inference-optimization-en/" style="display:none"><div class="post-content" id="post-content" itemprop="articleBody"><p>With the widespread application of LLMs across various industries, achieving efficient inference while maintaining model performance has become a key challenge.</p><a id="more"></a><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>Mainstream LLMs are based on the decoder-only architecture of transformers, with next token prediction as their core task. During inference, the model needs to generate each token sequentially, and this autoregressive generation characteristic increases inference latency. In terms of parameter count, models can be divided into different scales, and even small models have high requirements for computational and memory resources.</p><p>To address these challenges, academia and industry have proposed various optimization solutions. This article introduces several key technologies related to LLM inference acceleration. Corrections and suggestions are welcome.</p><h2 id="Main-Content"><a href="#Main-Content" class="headerlink" title="Main Content"></a>Main Content</h2><h3 id="Inference-Phase-Overview"><a href="#Inference-Phase-Overview" class="headerlink" title="Inference Phase Overview"></a>Inference Phase Overview</h3><p>The inference phase is the process where a model receives input and produces output. Unlike the training phase, inference doesn’t require gradient computation and parameter updates, only forward propagation calculations.</p><p>During inference, due to the autoregressive generation characteristic, the model needs to generate tokens one by one. This leads to continuously increasing sequence lengths. For example:</p><ul><li>With an input length of 1000 tokens and needing to generate 100 new tokens, the actual sequence length processed is: 1000 + 1001 + … + 1100 = 106050 tokens</li><li><strong>Each new token needs to compute attention with all previous tokens</strong></li></ul><p>In a typical decoder-only architecture, there are multiple decoder blocks, and each token must pass through these blocks during inference. Each block contains 3 main computation modules: <strong>Self-Attention</strong>, <strong>FFN</strong>, and Layer Normalization (a basic component for ensuring numerical stability, which we’ll ignore in the following discussion).</p><figure class="image-bubble"><div class="img-lightbox"><div class="overlay"></div> <img src="https://cdn.jsdelivr.net/gh/SimonAKing/images/loading/4-min.gif" alt="image" title data-original="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization-en/111.png"></div><div class="image-caption">image</div></figure><p>The execution phases can be divided into prefill and decode stages:</p><table><thead><tr><th>Stage</th><th>Characteristics</th><th>Modules</th></tr></thead><tbody><tr><td>Prefill</td><td>• Compute-intensive phase with O(n²) complexity due to full attention computation across all input tokens<br>• Memory usage scales quadratically with sequence length due to attention matrix computation<br>• Can leverage hardware parallelism as all tokens are processed simultaneously</td><td>• Self-Attention: Computes complete attention patterns between all input tokens, generating and caching K/V representations<br>• FFN: Processes all input tokens through feed-forward networks in parallel</td></tr><tr><td>Decode</td><td>• Linear complexity O(n) per token, but inherently sequential due to autoregressive nature<br>• Memory growth is linear and predictable<br>• Limited parallelization potential due to token-by-token generation</td><td>• Self-Attention: Generates Q for new token only, leverages cached K/V, computes attention scores efficiently<br>• FFN: Processes only the newly generated token through feed-forward networks</td></tr></tbody></table><blockquote><p>Demonstration of the prefilling stage (a) and decoding stage (b)</p></blockquote><figure class="image-bubble"><div class="img-lightbox"><div class="overlay"></div> <img src="https://cdn.jsdelivr.net/gh/SimonAKing/images/loading/3-min.gif" alt="image" title data-original="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization-en/222.png"></div><div class="image-caption">image</div></figure><blockquote><p>Illustration of the memory variation through time (latency) during inference</p></blockquote><figure class="image-bubble"><div class="img-lightbox"><div class="overlay"></div> <img src="https://cdn.jsdelivr.net/gh/SimonAKing/images/loading/2-min.gif" alt="image" title data-original="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization-en/333.png"></div><div class="image-caption">image</div></figure><p>Performance metrics during inference include:</p><ul><li>TTFT (Time-To-First-Token): Generation time for the first token, mainly measuring Prefill stage performance</li><li>TPOT (Time-Per-Output-Token): Time to generate each token, mainly measuring Decode stage performance</li></ul><p>Let’s understand the inference process through a real example, query: “What are some recent practices related to Deepseek R1?”</p><table><thead><tr><th>Stage</th><th>Module</th><th>Processing</th><th>Result</th></tr></thead><tbody><tr><td>Prefill</td><td>Self-Attention</td><td>Words like “happened” and “practices” will have high attention scores with “Deepseek R1”, producing an n×n attention matrix</td><td>Output the first token with highest probability</td></tr><tr><td></td><td>FFN</td><td>Will search for pretrained knowledge about Deepseek in the learned knowledge base</td><td></td></tr><tr><td>Decode</td><td>Self-Attention</td><td>Use the generated new token q to understand the input kv, discover it’s asking about practices, understand the need for practice-related knowledge</td><td>Output subsequent tokens with highest probability</td></tr><tr><td></td><td>FFN</td><td>Will search for practice-related knowledge in the learned knowledge base</td></tr></tbody></table><p>In summary, key overheads during inference:</p><ol><li>Computational cost:<ol><li>Prefill needs to process all inputs, input has quadratic relationship with computation</li><li>Model parameter count keeps increasing</li><li>Decode has low GPU computation</li></ol></li><li>Storage cost:<ol><li>KV Cache grows linearly with input sequence length, needs to store K/V for all historical tokens in memory</li><li>FFN storage overhead is large, estimated to account for over 70% of parameters</li></ol></li><li>IO cost: decode generates only one token each time but needs to repeatedly call the model</li></ol><h3 id="Technology-Overview"><a href="#Technology-Overview" class="headerlink" title="Technology Overview"></a>Technology Overview</h3><p>The industry has proposed optimization solutions from different stages to address these challenges:</p><figure class="image-bubble"><div class="img-lightbox"><div class="overlay"></div> <img src="https://cdn.jsdelivr.net/gh/SimonAKing/images/loading/4-min.gif" alt="image" title data-original="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization-en/4.png"></div><div class="image-caption">image</div></figure><h4 id="Data-Level"><a href="#Data-Level" class="headerlink" title="Data Level"></a>Data Level</h4><h5 id="Input-Compression"><a href="#Input-Compression" class="headerlink" title="Input Compression"></a>Input Compression</h5><p>Compress input text and prune low-quality information while minimizing performance impact to improve efficiency. Specific works include:</p><ol><li><a href="https://llmlingua.com/" target="_blank" rel="external nofollow noopener noreferrer">LLMLingua Series</a>: Train small models to prune and compress original prompts<div class="image-row"><figure class="image-bubble row-image"><div class="img-lightbox"><div class="overlay"></div> <img src="https://cdn.jsdelivr.net/gh/SimonAKing/images/loading/2-min.gif" alt="image" title data-original="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization-en/5.png"></div><div class="image-caption">image</div></figure><figure class="image-bubble row-image"><div class="img-lightbox"><div class="overlay"></div> <img src="https://cdn.jsdelivr.net/gh/SimonAKing/images/loading/3-min.gif" alt="image" title data-original="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization-en/6.png"></div><div class="image-caption">image</div></figure></div> The approach transforms compression into a token classification problem, deciding whether to keep or discard each token. The task model is trained on a dataset distilled from large models.<figure class="image-bubble"><div class="img-lightbox"><div class="overlay"></div> <img src="https://cdn.jsdelivr.net/gh/SimonAKing/images/loading/2-min.gif" alt="image" title data-original="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization-en/7.png"></div><div class="image-caption">image</div></figure></li></ol><p>Other compression works in RAG scenarios:</p><ol start="2"><li><p><a href="https://arxiv.org/abs/2310.04408" target="_blank" rel="external nofollow noopener noreferrer">RECOMP</a>: Compresses retrieved documents into text summaries to improve language model performance while reducing computational costs.</p><figure class="image-bubble"><div class="img-lightbox"><div class="overlay"></div> <img src="https://cdn.jsdelivr.net/gh/SimonAKing/images/loading/1-min.gif" alt="image" title data-original="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization-en/8.png"></div><div class="image-caption">image</div></figure></li><li><p><a href="https://arxiv.org/abs/2405.13792" target="_blank" rel="external nofollow noopener noreferrer">xRAG</a>: Projects document embeddings directly into LLMs’ representation space through modal fusion</p><figure class="image-bubble"><div class="img-lightbox"><div class="overlay"></div> <img src="https://cdn.jsdelivr.net/gh/SimonAKing/images/loading/4-min.gif" alt="image" title data-original="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization-en/9.png"></div><div class="image-caption">image</div></figure></li><li><p>Conventional RAG, Rerank: Suitable for information-intensive scenarios, selecting highly relevant content to avoid garbage in garbage out</p><figure class="image-bubble"><div class="img-lightbox"><div class="overlay"></div> <img src="https://cdn.jsdelivr.net/gh/SimonAKing/images/loading/3-min.gif" alt="image" title data-original="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization-en/10.png"></div><div class="image-caption">image</div></figure><p></p><blockquote><p>Impact of irrelevant passages on RAG effectiveness <a href="https://arxiv.org/pdf/2410.05983" target="_blank" rel="external nofollow noopener noreferrer">https://arxiv.org/pdf/2410.05983</a></p></blockquote></li></ol><h5 id="Output-Planning"><a href="#Output-Planning" class="headerlink" title="Output Planning"></a>Output Planning</h5><p>Improve parallelism of long text output while minimizing performance impact to enhance efficiency. Specific works include:</p><ol><li><p><a href="https://arxiv.org/abs/2307.15337" target="_blank" rel="external nofollow noopener noreferrer">SOT</a>: SOT proposes an approach of parallel output after understanding intent, generating an outline for the answer, then generating in parallel for each point in the outline, significantly improving linear decoding performance. However, it cannot handle scenarios well where dependencies exist between different points. Based on this, they additionally trained a router model to judge whether dependencies exist, and parallel decoding is not performed when dependencies exist.</p><div class="image-row"><figure class="image-bubble row-image"><div class="img-lightbox"><div class="overlay"></div> <img src="https://cdn.jsdelivr.net/gh/SimonAKing/images/loading/2-min.gif" alt="image" title data-original="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization-en/11.png"></div><div class="image-caption">image</div></figure><figure class="image-bubble row-image"><div class="img-lightbox"><div class="overlay"></div> <img src="https://cdn.jsdelivr.net/gh/SimonAKing/images/loading/4-min.gif" alt="image" title data-original="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization-en/12.png"></div><div class="image-caption">image</div></figure></div></li><li><p><a href="https://arxiv.org/abs/2402.12280" target="_blank" rel="external nofollow noopener noreferrer">SGD</a>: Takes a step further from SOT by abstracting dependency relationships into a DAG (points with dependencies have connecting lines; independent points are separate), thus further improving effectiveness.</p><figure class="image-bubble"><div class="img-lightbox"><div class="overlay"></div> <img src="https://cdn.jsdelivr.net/gh/SimonAKing/images/loading/1-min.gif" alt="image" title data-original="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization-en/13.png"></div><div class="image-caption">image</div></figure></li></ol><h4 id="Model-Level"><a href="#Model-Level" class="headerlink" title="Model Level"></a>Model Level</h4><h5 id="Model-Structure-Optimization"><a href="#Model-Structure-Optimization" class="headerlink" title="Model Structure Optimization"></a>Model Structure Optimization</h5><ol><li><p>Attention Optimization: The core is reducing KV Cache and kernel function computations.</p><blockquote><p>Adding Su Shen’s explanation: Why is reducing KV Cache size so important? As we all know, LLM inference is generally performed on GPUs, and single GPU memory is limited. One part needs to be used to store model parameters and activation values from forward computation, which depends on model size and is constant once the model is selected. Another part needs to be used to store the model’s KV Cache, which depends not only on model size but also on input length, growing dynamically during inference. When the context length is long enough, its size becomes dominant and may exceed the total memory of one card or even one machine (8 cards). The principle for deploying models on GPUs is: if it can be deployed on one card, don’t use multiple cards; if it can be deployed on one machine, don’t use multiple machines. This is because “intra-card communication bandwidth &gt; inter-card communication bandwidth &gt; inter-machine communication bandwidth”. Due to the “bucket effect”, the more devices the model deployment spans, the more it is “dragged down” by inter-device communication bandwidth. In fact, even though single H100 intra-card SRAM to HBM bandwidth has reached 3TB/s, this speed is still the bottleneck for inference with Short Context, not to mention slower inter-card and inter-machine communication. Therefore, the fundamental purpose of reducing KV Cache is to achieve inference of longer Context on fewer devices, thereby achieving faster inference speed and lower inference cost.</p></blockquote><ol><li>MHA (Multi-Head Attention): The most basic multi-head attention mechanism, where each attention head has independent Q, K, V parameter matrices. Although it can fully capture information from different feature dimensions, it needs to store KV Cache for all heads, resulting in large storage overhead.</li><li>MQA (Multi-Query Attention): When computing attention, shared K/V is broadcast to each attention head, then each head still uses its independent Q with shared K/V to compute attention scores. KV Cache size is reduced to 1/h of original (h is number of heads). However, sharing KV may limit the model’s ability to capture diverse features.</li><li>GQA (Grouped-Query Attention): A compromise between MQA and MHA, dividing attention heads into groups with heads in the same group sharing KV. Compared to MQA it maintains better model performance while significantly reducing KV Cache compared to MHA. Representative works: <a href="https://arxiv.org/pdf/2401.02954" target="_blank" rel="external nofollow noopener noreferrer">DeepSeek V1</a>, <a href="https://scontent-nrt1-2.xx.fbcdn.net/v/t39.2365-6/453304228_1160109801904614_7143520450792086005_n.pdf?_nc_cat=108&amp;ccb=1-7&amp;_nc_sid=3c67a6&amp;_nc_ohc=ivumNlLcSY8Q7kNvgEYKQgM&amp;_nc_ht=scontent-nrt1-2.xx&amp;oh=00_AYDWnrIK1XXJ_bo1ueusjYFZjgJALgata0ZSOFgV6qJHHA&amp;oe=66BA8087" target="_blank" rel="external nofollow noopener noreferrer">LLaMa 3.1</a>, <a href="https://arxiv.org/pdf/2407.10671" target="_blank" rel="external nofollow noopener noreferrer">Qwen2</a>.</li><li>MLA (Multi-head Latent Attention): A new attention mechanism introduced by <a href="https://arxiv.org/pdf/2405.04434" target="_blank" rel="external nofollow noopener noreferrer">DeepSeek V2</a> through introducing latent variables. It significantly reduces KV cache size by compressing keys (Key) and values (Value) to a low-dimensional latent space. Compared to standard attention mechanisms, it reportedly reduces KV cache by about 93.3%.<div class="image-row"><figure class="image-bubble row-image"><div class="img-lightbox"><div class="overlay"></div> <img src="https://cdn.jsdelivr.net/gh/SimonAKing/images/loading/1-min.gif" alt="image" title data-original="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization-en/14.png"></div><div class="image-caption">image</div></figure><figure class="image-bubble row-image"><div class="img-lightbox"><div class="overlay"></div> <img src="https://cdn.jsdelivr.net/gh/SimonAKing/images/loading/2-min.gif" alt="image" title data-original="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization-en/15.png"></div><div class="image-caption">image</div></figure></div></li></ol></li><li><p>MOE (Mixture-of-Expert): An advanced neural architecture that replaces the traditional monolithic FFN with a dynamic routing system. The architecture consists of:</p><ul><li>A gate network that learns to route tokens to the most relevant expert networks</li><li>Multiple specialized expert networks (typically 8-32) that process different aspects of the input</li><li>A load balancing mechanism to ensure efficient utilization of experts</li></ul><p>This design offers several advantages:</p><ul><li>Increased model capacity without proportional increase in computation cost</li><li>Dynamic specialization where different experts can focus on different types of tokens/tasks</li><li>Improved inference efficiency through selective expert activation</li></ul><p>The approach originated from the 1991 paper <a href="https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf" target="_blank" rel="external nofollow noopener noreferrer">Adaptive Mixture of Local Experts</a> and has seen renewed interest in modern LLMs. Starting with Mistral’s Mixtral 8x7B, MOE has become a mainstream trend in 2024, with DeepSeek’s implementations showcasing increasingly sophisticated architectures across their model versions. Key challenges in MOE implementation include:</p><ul><li>Balanced token routing to prevent expert overload</li><li>Ensuring expert specialization without knowledge conflicts</li><li>Maintaining routing efficiency at inference time<div class="image-row"><figure class="image-bubble row-image"><div class="img-lightbox"><div class="overlay"></div> <img src="https://cdn.jsdelivr.net/gh/SimonAKing/images/loading/4-min.gif" alt="image" title data-original="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization-en/16.png"></div><div class="image-caption">image</div></figure><figure class="image-bubble row-image"><div class="img-lightbox"><div class="overlay"></div> <img src="https://cdn.jsdelivr.net/gh/SimonAKing/images/loading/1-min.gif" alt="image" title data-original="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization-en/17.png"></div><div class="image-caption">image</div></figure></div></li></ul></li></ol><h5 id="Model-Compression"><a href="#Model-Compression" class="headerlink" title="Model Compression"></a>Model Compression</h5><p>Model compression aims to reduce resource overhead by reducing model complexity (including parameter count, computation, and memory usage) while maintaining model performance as much as possible. Some representative measures include:</p><figure class="image-bubble"><div class="img-lightbox"><div class="overlay"></div> <img src="https://cdn.jsdelivr.net/gh/SimonAKing/images/loading/2-min.gif" alt="image" title data-original="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization-en/18.png"></div><div class="image-caption">image</div></figure><ul><li><p>Quantization: Quantization reduces the precision of model weights and activations. Most models are trained with 32 or 16-bit precision, where each parameter and activation element occupies 32 or 16 bits of memory (single precision floating point). Obviously, we can try to use fewer bits to represent weights. Common methods include post-training quantization, directly converting numerical precision after model training, simple operation but may reduce accuracy; and quantization-aware training, simulating quantization effects during training to let the model adapt to low precision in advance, resulting in less accuracy loss. Benefits include significantly reducing model size and improving computation speed. Focus on activation value quantization in prefill stage; focus on weight quantization in decode stage.</p><figure class="image-bubble"><div class="img-lightbox"><div class="overlay"></div> <img src="https://cdn.jsdelivr.net/gh/SimonAKing/images/loading/3-min.gif" alt="image" title data-original="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization-en/19.png"></div><div class="image-caption">image</div></figure></li><li><p><a href="https://arxiv.org/pdf/1503.02531" target="_blank" rel="external nofollow noopener noreferrer">Knowledge Distillation</a>: Knowledge distillation is a compression method that transfers knowledge from large models (teacher models) to small models (student models). Common methods include post-training distillation, directly using teacher model to generate data to train student model, which can compress the model to 40-60% of original size while maintaining most performance.</p><figure class="image-bubble"><div class="img-lightbox"><div class="overlay"></div> <img src="https://cdn.jsdelivr.net/gh/SimonAKing/images/loading/1-min.gif" alt="image" title data-original="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization-en/20.png"></div><div class="image-caption">image</div></figure></li><li><p>Pruning: Compress model by removing unimportant connections or structures. Common methods include structured pruning (removing entire layers or heads) and unstructured pruning (removing individual weights), can reduce parameters by 30-50% with minimal performance loss.</p></li><li><p>Low-rank Decomposition: Decompose large weight matrices into products of several small matrices to reduce parameter count and computation. Common methods include SVD decomposition and LoRA etc. Among them, LoRA achieves parameter-efficient fine-tuning by adding low-rank matrices beside original weights, becoming one of the mainstream model compression methods.</p></li></ul><h5 id="Decoding-Optimization"><a href="#Decoding-Optimization" class="headerlink" title="Decoding Optimization"></a>Decoding Optimization</h5><p>The most time-consuming part of inference stage is the autoregressive generation process. Decoding optimization aims to accelerate token generation through parallelization or prediction. Some representative measures include:</p><ol><li><p><a href="https://arxiv.org/abs/2302.01318" target="_blank" rel="external nofollow noopener noreferrer">Speculative Decoding</a>: An innovative decoding technique for autoregressive large models aimed at improving decoding efficiency without affecting output quality. The core idea includes using a smaller model (Draft Model) to effectively predict several subsequent tokens, then using the large model to verify these predictions in parallel. This method aims to enable the large model to generate multiple tokens in the time range usually required for a single inference.</p><figure class="image-bubble"><div class="img-lightbox"><div class="overlay"></div> <img src="https://cdn.jsdelivr.net/gh/SimonAKing/images/loading/1-min.gif" alt="image" title data-original="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization-en/21.png"></div><div class="image-caption">image</div></figure></li><li><p>Skip-token Decoding: Representative work SGLang, core idea is to skip predictable parts during generation by analyzing deterministic features of text structure. Instead of generating token by token, it can directly jump to the next position requiring inference at positions with high determinism, greatly improving generation efficiency.</p><figure class="image-bubble"><div class="img-lightbox"><div class="overlay"></div> <img src="https://cdn.jsdelivr.net/gh/SimonAKing/images/loading/2-min.gif" alt="image" title data-original="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization-en/22.png"></div><div class="image-caption">image</div></figure></li><li><p>Constrained Decoding: Representative work Outlines, Outlines converts JSON Schema into regular expressions, then builds finite state machines (FSM) based on these regular expressions, then filters tokens in real-time during generation to ensure output always conforms to predefined format requirements. This approach is particularly suitable for scenarios requiring strict output format control.</p><figure class="image-bubble"><div class="img-lightbox"><div class="overlay"></div> <img src="https://cdn.jsdelivr.net/gh/SimonAKing/images/loading/2-min.gif" alt="image" title data-original="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization-en/23.png"></div><div class="image-caption">image</div></figure></li><li><p>Structured Decoding: Representative work Guidance, innovation lies in dividing generation tasks into fixed structure and dynamic content categories and adopting different processing strategies. By identifying template parts in text, it can significantly reduce the amount of content that needs to be actually generated, thereby improving overall efficiency.</p><figure class="image-bubble"><div class="img-lightbox"><div class="overlay"></div> <img src="https://cdn.jsdelivr.net/gh/SimonAKing/images/loading/4-min.gif" alt="image" title data-original="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization-en/24.png"></div><div class="image-caption">image</div></figure></li></ol><h4 id="System-Level"><a href="#System-Level" class="headerlink" title="System Level"></a>System Level</h4><h5 id="KV-Cache"><a href="#KV-Cache" class="headerlink" title="KV Cache"></a>KV Cache</h5><p>KV Cache optimization is critical for LLM inference efficiency as it represents a significant memory bottleneck. The optimization focuses on two key aspects:</p><ol><li><p>Sparsification Compression:</p><ul><li>Approach: Strategically select and retain only the most informative tokens while pruning less important ones</li><li>Implementation: Uses attention scoring to identify key tokens</li><li>Example:<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">Original sequence (dense):</span><br><span class="line">[T1, T2, T3, ..., T128K]  // Full 128K token sequence</span><br><span class="line"></span><br><span class="line">After sparsification (sparse):</span><br><span class="line">[T1, T512, T1024, ..., T128K]  // Only ~1K tokens retained</span><br><span class="line">- Retains tokens with high attention scores</span><br><span class="line">- Maintains semantic coherence</span><br><span class="line">- Reduces memory footprint by &gt;90%</span><br></pre></td></tr></tbody></table></figure></li></ul></li><li><p>Quantization Compression:</p><ul><li>Approach: Reduce numerical precision of stored K/V values</li><li>Implementation methods:<ul><li>Post-training quantization (e.g., FP16 → INT8)</li><li>Dynamic quantization during inference</li><li>Mixed-precision storage strategies</li></ul></li><li>Benefits:<ul><li>2-4x memory reduction with minimal accuracy impact</li><li>Improved cache hit rates</li><li>Better hardware utilization</li></ul></li></ul></li></ol><p>These optimization strategies can be combined and tuned based on specific requirements:</p><ul><li>Long-context scenarios benefit more from sparsification</li><li>Latency-sensitive applications might prefer quantization</li><li>Production systems often use a hybrid approach</li></ul><p>The optimization process typically follows this workflow:</p><figure class="image-bubble"><div class="img-lightbox"><div class="overlay"></div> <img src="https://cdn.jsdelivr.net/gh/SimonAKing/images/loading/4-min.gif" alt="image" title data-original="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization-en/25.webp"></div><div class="image-caption">image</div></figure><h5 id="PagedAttention"><a href="#PagedAttention" class="headerlink" title="PagedAttention"></a>PagedAttention</h5><p>Traditional inference frameworks adopt static memory allocation strategy: pre-allocating fixed size memory blocks according to batch_size × max_seq_len, resulting in low memory utilization.</p><figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">// Traditional static allocation</span><br><span class="line">Pre-allocated memory size = 4(batch_size) × 2048(max_seq_len) × 2(bytes/token)</span><br><span class="line">Actual usage:</span><br><span class="line">- Request1: uses 256 tokens</span><br><span class="line">- Request2: uses 512 tokens</span><br><span class="line">- Request3: uses 128 tokens</span><br><span class="line">Result: Large amount of memory space wasted</span><br></pre></td></tr></tbody></table></figure> vllm borrows virtual memory paging idea from operating systems, proposing PagedAttention technology to achieve dynamic allocation of KV Cache memory.<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">// PagedAttention dynamic allocation</span><br><span class="line">Memory page size = 16KB</span><br><span class="line">Request1: dynamically allocate 2 pages (32KB)</span><br><span class="line">Request2: dynamically allocate 4 pages (64KB)</span><br><span class="line">Request3: dynamically allocate 1 page (16KB)</span><br><span class="line">Advantage: Allocate on demand, improve memory utilization</span><br></pre></td></tr></tbody></table></figure> This dynamic management mechanism allows memory resources to be used more efficiently, particularly suitable for handling multi-request scenarios with varying lengths.<p></p><figure class="image-bubble"><div class="img-lightbox"><div class="overlay"></div> <img src="https://cdn.jsdelivr.net/gh/SimonAKing/images/loading/2-min.gif" alt="image" title data-original="https://cdn.jsdelivr.net/gh/SimonAKing/blog/llm-inference-optimization-en/26.png"></div><div class="image-caption">image</div></figure><h5 id="Prefix-Cache"><a href="#Prefix-Cache" class="headerlink" title="Prefix Cache"></a>Prefix Cache</h5><p>Limitation of traditional PagedAttention: KV Cache can only be reused within single request, cannot be shared across requests, which is inefficient in multi-round dialogue scenarios. Example:</p><figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">// Traditional way</span><br><span class="line">Dialogue round 1:</span><br><span class="line">  Prompt: "Hello, please introduce Beijing"</span><br><span class="line">  → Calculate complete KV Cache</span><br><span class="line"></span><br><span class="line">Dialogue round 2:</span><br><span class="line">  Prompt: "Hello, please introduce the Forbidden City in Beijing"</span><br><span class="line">  → Recalculate KV Cache (even though contains lots of repeated content)</span><br></pre></td></tr></tbody></table></figure> Prefix Caching optimization: Achieve cross-request reuse by caching KV Cache of system prompts and dialogue history, reducing Time To First Token (TTFT). RadixAttention proposed by SGLang achieves automatic KV Cache reuse through three key steps:<p></p><ol><li>Cache retention<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">After request completion:</span><br><span class="line">KV Cache → Retained in GPU memory</span><br><span class="line">Token sequence → Stored in RadixTree</span><br></pre></td></tr></tbody></table></figure></li><li>Prefix matching<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">New request: "Hello, please introduce the Forbidden City in Beijing"</span><br><span class="line">↓</span><br><span class="line">RadixTree matching</span><br><span class="line">↓</span><br><span class="line">Hit: KV Cache of "Hello, please introduce Beijing"</span><br><span class="line">↓</span><br><span class="line">Only need to calculate new part: "the Forbidden City"</span><br></pre></td></tr></tbody></table></figure></li><li>Cache management<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">Strategy: LRU (Least Recently Used)</span><br><span class="line">When memory is insufficient:</span><br><span class="line">- Identify least recently used KV Cache</span><br><span class="line">- Prioritize releasing this part of memory</span><br></pre></td></tr></tbody></table></figure> This optimization is particularly suitable for scenarios like multi-round dialogues, can significantly improve model response speed and resource utilization efficiency.</li></ol><h5 id="Others"><a href="#Others" class="headerlink" title="Others"></a>Others</h5><p>There are many other optimization approaches at system level, such as tensor parallelism, pipeline parallelism, distributed inference etc. Since these optimization solutions involve relatively complex system architecture design, we won’t discuss them in detail here. Interested readers can refer to relevant technical documentation for deeper understanding.</p><hr><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Through in-depth discussion of LLM inference acceleration technologies, we can see:</p><ol><li>Technical Architecture and Challenges<ul><li>Inference process divided into two main stages: Prefill and Decode</li><li>Core challenges:<ul><li>Computational cost: input scale, parameter count, autoregressive nature</li><li>Storage cost: KV Cache, FFN parameter storage</li><li>IO cost: frequent model calls</li></ul></li></ul></li><li>Optimization Solutions (Multi-level Technology Stack)<ul><li>Data level optimization:<ul><li>Input compression: LLMLingua, RECOMP, xRAG etc.</li><li>Output planning: SOT, SGD and other parallel output technologies</li></ul></li><li>Model level optimization:<ul><li>Structure optimization: attention mechanisms (MHA, MQA, GQA, MLA), MOE architecture</li><li>Model compression: quantization, knowledge distillation, pruning, low-rank decomposition</li><li>Decoding optimization: speculative decoding, skip-token decoding, constrained decoding etc.</li></ul></li><li>System level optimization:<ul><li>KV Cache optimization: sparsification compression, quantization compression</li><li>PagedAttention: dynamic memory management</li><li>Prefix Cache: cross-request KV reuse As LLMs continue to expand their applications across various fields, inference acceleration technologies also continue to evolve. From early simple optimizations to current multi-level comprehensive optimization solutions, technology keeps breaking through. The development of these optimization technologies enables LLMs to serve practical application scenarios more efficiently, laying an important foundation for the popularization of AI applications. In the future, with the advancement of hardware technology and algorithm innovation, we expect to see more breakthrough inference acceleration solutions.</li></ul></li></ul></li></ol><p>References:</p><ol><li><a href="https://arxiv.org/abs/2404.14294" target="_blank" rel="external nofollow noopener noreferrer">A Survey on Efficient Inference for Large Language Models</a> Very helpful, recommended</li><li><a href="https://arxiv.org/abs/2402.09748" target="_blank" rel="external nofollow noopener noreferrer">Model Compression and Efficient Inference for Large Language Models: A Survey</a></li><li><a href="https://developer.nvidia.com/blog/mastering-llm-techniques-inference-optimization/" target="_blank" rel="external nofollow noopener noreferrer">Mastering LLM Techniques: Inference Optimization</a></li><li><a href="https://zhuanlan.zhihu.com/p/689773196" target="_blank" rel="external nofollow noopener noreferrer">AI Large Model Inference Process and Optimization Technologies</a></li><li><a href="https://vgel.me/posts/faster-inference/" target="_blank" rel="external nofollow noopener noreferrer">How to make LLMs go fast</a></li><li><a href="https://www.yidoo.xyz/" target="_blank" rel="external nofollow noopener noreferrer">Yidoo Blog</a></li><li><a href="https://mp.weixin.qq.com/s/WFJxnTF9fGIIXPA7GQ5V2w" target="_blank" rel="external nofollow noopener noreferrer">Detailed Discussion on DeepSeek MoE Related Technical Development</a></li><li><a href="https://mp.weixin.qq.com/s/yCczYU0po0PvPTa-eh2pfg" target="_blank" rel="external nofollow noopener noreferrer">The Ultimate Trade-off Between Cache and Effect: From MHA, MQA, GQA to MLA</a></li><li><a href="https://arxiv.org/pdf/1503.02531" target="_blank" rel="external nofollow noopener noreferrer">A Survey on Model Compression for Large Language Models</a></li></ol><p>Feel free to share articles from this site, please credit the author and source <a href="http://simonaking.com">SimonAKing</a>.</p></div><div class="recommended_posts"><div class="recommended_wrap"><h2 class="recommended_title">相关文章</h2><ul class="popular-posts"><li class="popular-posts-item"><div class="popular-posts-title"> <a href="/blog/llm-inference-optimization/" rel="bookmark">聊聊大模型推理加速：从数据到系统的技术概要</a></div></li><li class="popular-posts-item"><div class="popular-posts-title"> <a href="/blog/rerank/" rel="bookmark">聊聊 Rerank：从 BERT 到大模型的技术旅程</a></div></li></ul></div></div><blockquote class="post-copyright"><div class="content"> <span class="post-time">最后更新：<time datetime="2025-05-11T15:32:57.684Z">2025-05-11 23:32:57</time></span> 原文链接：<a href="/blog/llm-inference-optimization-en/" target="_blank" rel="external">https://simonaking.com/blog/llm-inference-optimization-en/</a></div><footer><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/blog/tags/Inference-Optimization/" rel="tag">Inference Optimization</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/blog/tags/LLM/" rel="tag">LLM</a></li></ul><div> <img itemprop="image" lass="copyimg" src="https://cdn.jsdelivr.net/gh/SimonAKing/images/blog/avatar.jpg" alt="SimonAKing"> <a itemprop="author" itemscope itemtype="http://schema.org/Person" href="/blog">SimonAKing</a></div></footer></blockquote></div><section class="comments" id="comments"><div id="disqus_thread"></div></section></article></div></main><div class="mask" id="mask"></div><a href="javascript:;" id="gotop" class="fade-scale waves-effect waves-circle waves-light"><span class="icon icon-lg icon-chevronup"></span></a><div class="search-panel in_post" id="search-panel"><ul class="search-result" id="search-result"></ul></div><template id="search-tpl"><li class="item waves-block waves-effect" onclick="location.href='{path}'" data-path="{path}"><div class="title ellipsis" title="{title}">{title}</div></li></template><div id="color-picker" class="page-modal"><a class="close" href="javascript:;"><i class="icon icon-close2"></i></a><br><h4><i class="icon icon-quote-left"></i> <span>RGB调色实验室 🎨</span><i class="icon icon-quote-right"></i></h4><div id="color-picker-container"></div></div><div id="wechat" class="page-modal wechat-wrap"><a class="close" href="javascript:;"><i class="icon icon-close2"></i></a><br><h4 class="wechat-title"> <span style="font-size: 18px">欢迎添加我的微信，进群交流、学习<br><i class="icon icon-quote-left"></i> simonaking<i class="icon icon-quote-right"></i></span></h4> <img id="wechat_img" src="https://cdn.jsdelivr.net/gh/SimonAKing/images/blog/wx.png" alt="欢迎您的关注！" style="margin-top: 5%"></div><script src="https://simonaking.com/registerSW.js?v=7d04115230169"></script><script src="https://cdn.jsdelivr.net/gh/SimonAKing/js/log.min.js"></script><script src="https://cdn.jsdelivr.net/npm/node-waves@0.7.6/src/js/waves.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/headroom.js@0.12.0/dist/headroom.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@jaames/iro@5.2.3/dist/iro.min.js"></script><script src="https://cdn.jsdelivr.net/gh/SimonAKing/blog@7d04115230169/js/particles-585dd6bbf8.js"></script><script src="https://cdn.jsdelivr.net/gh/SimonAKing/blog@7d04115230169/js/main-c5641580d9.js"></script><script src="https://cdn.jsdelivr.net/gh/SimonAKing/blog@7d04115230169/js/script-66d90ba09e.js"></script><script src="https://cdn.jsdelivr.net/gh/SimonAKing/blog@7d04115230169/js/module/hover-effect-e72a114ce3.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@5.1.0/instantpage.min.js"></script><script src="https://cdn.jsdelivr.net/gh/SimonAKing/blog@7d04115230169/js/module/post-6d3f729af5.js"></script><script defer src="https://cdn.jsdelivr.net/gh/SimonAKing/blog@7d04115230169/js/module/comment-5cad405b01.js"></script><script defer src="https://cdn.jsdelivr.net/gh/SimonAKing/js/translate-ff7607fcb8.js"></script></body></html>